{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5966927,"sourceType":"datasetVersion","datasetId":3421204}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nROOT = \"/kaggle/input/indoor-small-object-dataset/ISOD\"\n\nfor root, dirs, files in os.walk(ROOT):\n    level = root.replace(ROOT, \"\").count(os.sep)\n    \n    # limit depth: root + sub + sub-sub\n    if level > 4:\n        continue\n\n    indent = \"│   \" * level\n    print(f\"{indent}├── {os.path.basename(root)}/\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nimport json\nfrom pathlib import Path\nfrom collections import defaultdict, Counter\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageFile\nimport matplotlib.pyplot as plt\n\nImageFile.LOAD_TRUNCATED_IMAGES = False\n\nKAGGLE_INPUT_ROOT = Path(\"/kaggle/input\")\nWORK_ROOT = Path(\"/kaggle/working\")\nRESULTS_DIR = WORK_ROOT / \"results\" / \"01_data_integrity_and_profile\"\nRESULTS_DIR.mkdir(parents=True, exist_ok=True)\n\nIMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"}\n\ndef find_isod_data_root():\n    candidates = []\n    direct = KAGGLE_INPUT_ROOT / \"ISOD\" / \"data\"\n    if direct.exists():\n        candidates.append(direct)\n    for p in KAGGLE_INPUT_ROOT.glob(\"*\"):\n        if p.is_dir():\n            cand = p / \"ISOD\" / \"data\"\n            if cand.exists():\n                candidates.append(cand)\n    for p in KAGGLE_INPUT_ROOT.rglob(\"ISOD\"):\n        if p.is_dir():\n            cand = p / \"data\"\n            if cand.exists():\n                candidates.append(cand)\n    candidates = sorted(set(candidates), key=lambda x: str(x))\n    if not candidates:\n        raise FileNotFoundError(\"Could not find ISOD/data under /kaggle/input\")\n    return candidates[0]\n\nDATA_ROOT = find_isod_data_root()\nWORK_DATA_ROOT = DATA_ROOT\n\ndef list_files_in_dir(d: Path):\n    if not d.exists():\n        return []\n    files = []\n    for ext in IMG_EXTS:\n        files.extend(d.glob(f\"*{ext}\"))\n        files.extend(d.glob(f\"*{ext.upper()}\"))\n    return sorted(set(files), key=lambda x: x.name)\n\ndef stem_id(p: Path):\n    return p.stem\n\ndef infer_bitdepth_pil(img: Image.Image, arr: np.ndarray):\n    if arr is not None:\n        if arr.dtype == np.uint8:\n            return 8\n        if arr.dtype == np.uint16:\n            return 16\n        if arr.dtype == np.uint32:\n            return 32\n        if arr.dtype == np.int32:\n            return 32\n        if arr.dtype == np.float32:\n            return 32\n        if arr.dtype == np.float64:\n            return 64\n    mode = img.mode\n    if mode in [\"1\"]:\n        return 1\n    if mode in [\"L\", \"P\", \"RGB\", \"RGBA\", \"CMYK\", \"YCbCr\"]:\n        return 8\n    if mode in [\"I;16\", \"I;16B\", \"I;16L\", \"I;16N\"]:\n        return 16\n    if mode in [\"I\"]:\n        return 32\n    if mode in [\"F\"]:\n        return 32\n    return -1\n\ndef safe_read_image(path: Path):\n    try:\n        with Image.open(path) as img:\n            img.load()\n            arr = np.array(img)\n            bitdepth = infer_bitdepth_pil(img, arr)\n            shape = arr.shape\n        return True, shape, bitdepth, \"\"\n    except Exception as e:\n        return False, None, None, f\"{type(e).__name__}: {str(e)[:200]}\"\n\ndef file_size_kb(path: Path):\n    try:\n        return os.path.getsize(path) / 1024.0\n    except Exception:\n        return np.nan\n\ndef mask_object_area(mask_arr: np.ndarray):\n    if mask_arr is None:\n        return np.nan\n    if mask_arr.ndim == 3:\n        m = np.any(mask_arr > 0, axis=2)\n        return int(np.count_nonzero(m))\n    return int(np.count_nonzero(mask_arr > 0))\n\ndef depth_valid_ratio(depth_arr: np.ndarray):\n    if depth_arr is None:\n        return np.nan\n    valid = np.count_nonzero(depth_arr > 0)\n    total = depth_arr.size\n    if total == 0:\n        return np.nan\n    return float(valid) / float(total)\n\ndef deterministic_sample_id(site_id: str, stem: str):\n    return f\"{site_id}__{stem}\"\n\ndef try_parse_labels(label_dir: Path, stem: str, label_parse_notes: list):\n    if not label_dir.exists():\n        return None\n    for ext in [\".txt\", \".json\", \".xml\", \".csv\"]:\n        lp = label_dir / f\"{stem}{ext}\"\n        if lp.exists():\n            try:\n                if ext == \".txt\":\n                    txt = lp.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n                    tok = re.split(r\"\\s+\", txt)[0] if txt else \"\"\n                    if tok.isdigit():\n                        return tok\n                    if tok:\n                        return tok\n                if ext == \".json\":\n                    obj = json.loads(lp.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n                    for k in [\"class\", \"label\", \"category\", \"category_id\", \"cls\"]:\n                        if isinstance(obj, dict) and k in obj:\n                            return str(obj[k])\n                    if isinstance(obj, list) and obj:\n                        for k in [\"class\", \"label\", \"category\", \"category_id\", \"cls\"]:\n                            if isinstance(obj[0], dict) and k in obj[0]:\n                                return str(obj[0][k])\n            except Exception as e:\n                label_parse_notes.append(f\"{lp}: {type(e).__name__}\")\n                return None\n    return None\n\ndef safe_percentile(x, q):\n    if len(x) == 0:\n        return np.nan\n    return float(np.percentile(np.array(x), q))\n\nplt.rcParams.update({\n    \"font.size\": 10,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 10,\n    \"xtick.labelsize\": 9,\n    \"ytick.labelsize\": 9,\n    \"legend.fontsize\": 9,\n})\n\ndef save_fig(path: Path, dpi=300):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    plt.tight_layout()\n    plt.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n    plt.close()\n\nsite_dirs = sorted([p for p in DATA_ROOT.iterdir() if p.is_dir()], key=lambda x: x.name)\nif not site_dirs:\n    raise RuntimeError(f\"No site folders found under: {DATA_ROOT}\")\n\nrecords = []\nissues_rows = []\n\nglobal_rgb_shapes = []\nglobal_depth_shapes = []\nglobal_mask_shapes = []\nglobal_rgb_bitdepths = []\nglobal_depth_bitdepths = []\nglobal_mask_bitdepths = []\nglobal_object_areas = []\nglobal_depth_valid_ratios = []\n\nsite_valid_counts = Counter()\nsite_total_counts = Counter()\nsite_object_areas = defaultdict(list)\nsite_depth_valid_ratios = defaultdict(list)\n\nlabel_class_counter = Counter()\nlabel_parse_notes = []\n\nfor site_path in site_dirs:\n    site_id = site_path.name\n\n    rgb_dir = site_path / \"rgb\"\n    depth_dir = site_path / \"depth\"\n    mask_dir = site_path / \"mask\"\n    label_dir = site_path / \"label\"\n\n    rgb_files = list_files_in_dir(rgb_dir)\n    depth_files = list_files_in_dir(depth_dir)\n    mask_files = list_files_in_dir(mask_dir)\n\n    rgb_map = {stem_id(p): p for p in rgb_files}\n    depth_map = {stem_id(p): p for p in depth_files}\n    mask_map = {stem_id(p): p for p in mask_files}\n\n    all_stems = sorted(set(rgb_map.keys()) | set(depth_map.keys()) | set(mask_map.keys()))\n\n    for stem in all_stems:\n        sample_id = deterministic_sample_id(site_id, stem)\n        site_total_counts[site_id] += 1\n\n        rgb_path = rgb_map.get(stem, None)\n        depth_path = depth_map.get(stem, None)\n        mask_path = mask_map.get(stem, None)\n\n        notes = []\n        status = \"valid\"\n\n        if rgb_path is None:\n            status = \"missing_rgb\"\n            notes.append(\"RGB missing\")\n        if depth_path is None:\n            if status == \"valid\":\n                status = \"missing_depth\"\n            notes.append(\"Depth missing\")\n        if mask_path is None:\n            if status == \"valid\":\n                status = \"missing_mask\"\n            notes.append(\"Mask missing\")\n\n        rgb_shape = depth_shape = mask_shape = None\n        rgb_bit = depth_bit = mask_bit = None\n        rgb_ok = depth_ok = mask_ok = False\n\n        rgb_size_kb = depth_size_kb = mask_size_kb = np.nan\n\n        if rgb_path is not None:\n            rgb_size_kb = file_size_kb(rgb_path)\n            rgb_ok, rgb_shape, rgb_bit, rgb_note = safe_read_image(rgb_path)\n            if not rgb_ok:\n                if status == \"valid\":\n                    status = \"unreadable_rgb\"\n                notes.append(f\"RGB unreadable: {rgb_note}\")\n\n        if depth_path is not None:\n            depth_size_kb = file_size_kb(depth_path)\n            depth_ok, depth_shape, depth_bit, depth_note = safe_read_image(depth_path)\n            if not depth_ok:\n                if status == \"valid\":\n                    status = \"unreadable_depth\"\n                notes.append(f\"Depth unreadable: {depth_note}\")\n\n        if mask_path is not None:\n            mask_size_kb = file_size_kb(mask_path)\n            mask_ok, mask_shape, mask_bit, mask_note = safe_read_image(mask_path)\n            if not mask_ok:\n                if status == \"valid\":\n                    status = \"unreadable_mask\"\n                notes.append(f\"Mask unreadable: {mask_note}\")\n\n        obj_area = np.nan\n        d_valid_ratio = np.nan\n\n        if mask_ok and mask_path is not None:\n            with Image.open(mask_path) as mimg:\n                mimg.load()\n                marr = np.array(mimg)\n            obj_area = mask_object_area(marr)\n\n        if depth_ok and depth_path is not None:\n            with Image.open(depth_path) as dimg:\n                dimg.load()\n                darr = np.array(dimg)\n            d_valid_ratio = depth_valid_ratio(darr)\n\n        parsed_label = try_parse_labels(label_dir, stem, label_parse_notes)\n        if parsed_label is not None:\n            label_class_counter[parsed_label] += 1\n\n        triplet_exists = (rgb_path is not None) and (depth_path is not None) and (mask_path is not None)\n        triplet_readable = rgb_ok and depth_ok and mask_ok\n\n        if triplet_exists and triplet_readable:\n            status = \"valid\"\n            site_valid_counts[site_id] += 1\n\n            if rgb_shape is not None:\n                global_rgb_shapes.append(rgb_shape)\n                global_rgb_bitdepths.append(rgb_bit)\n            if depth_shape is not None:\n                global_depth_shapes.append(depth_shape)\n                global_depth_bitdepths.append(depth_bit)\n            if mask_shape is not None:\n                global_mask_shapes.append(mask_shape)\n                global_mask_bitdepths.append(mask_bit)\n\n            if not np.isnan(obj_area):\n                global_object_areas.append(obj_area)\n                site_object_areas[site_id].append(obj_area)\n\n            if not np.isnan(d_valid_ratio):\n                global_depth_valid_ratios.append(d_valid_ratio)\n                site_depth_valid_ratios[site_id].append(d_valid_ratio)\n        else:\n            issue_type = status if status != \"valid\" else \"incomplete_or_unreadable_triplet\"\n            details = \"; \".join(notes) if notes else \"Triplet incomplete or unreadable\"\n            issues_rows.append({\n                \"sample_id\": sample_id,\n                \"site_id\": site_id,\n                \"issue_type\": issue_type,\n                \"details\": details\n            })\n\n        records.append({\n            \"sample_id\": sample_id,\n            \"site_id\": site_id,\n            \"rgb_path\": str(rgb_path) if rgb_path is not None else \"\",\n            \"depth_path\": str(depth_path) if depth_path is not None else \"\",\n            \"mask_path\": str(mask_path) if mask_path is not None else \"\",\n            \"rgb_shape\": str(rgb_shape) if rgb_shape is not None else \"\",\n            \"depth_shape\": str(depth_shape) if depth_shape is not None else \"\",\n            \"mask_shape\": str(mask_shape) if mask_shape is not None else \"\",\n            \"rgb_bitdepth\": int(rgb_bit) if rgb_bit is not None else \"\",\n            \"depth_bitdepth\": int(depth_bit) if depth_bit is not None else \"\",\n            \"mask_bitdepth\": int(mask_bit) if mask_bit is not None else \"\",\n            \"file_size_kb_rgb\": float(rgb_size_kb) if not np.isnan(rgb_size_kb) else \"\",\n            \"file_size_kb_depth\": float(depth_size_kb) if not np.isnan(depth_size_kb) else \"\",\n            \"file_size_kb_mask\": float(mask_size_kb) if not np.isnan(mask_size_kb) else \"\",\n            \"status\": status,\n            \"notes\": \"; \".join(notes)\n        })\n\ndf_manifest = pd.DataFrame(records).sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\n\nif len(issues_rows) == 0:\n    df_issues = pd.DataFrame(columns=[\"sample_id\", \"site_id\", \"issue_type\", \"details\"])\nelse:\n    df_issues = pd.DataFrame(issues_rows).sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\n\nTAB_01_MANIFEST = RESULTS_DIR / \"tab_01_dataset_manifest_full.csv\"\nTAB_04_ISSUES = RESULTS_DIR / \"tab_04_corrupt_or_missing_samples.csv\"\ndf_manifest.to_csv(TAB_01_MANIFEST, index=False)\ndf_issues.to_csv(TAB_04_ISSUES, index=False)\n\ndf_valid = df_manifest[df_manifest[\"status\"] == \"valid\"].copy()\nTAB_01_MANIFEST_VALID = RESULTS_DIR / \"tab_01_dataset_manifest_valid_only.csv\"\ndf_valid.to_csv(TAB_01_MANIFEST_VALID, index=False)\n\ntotal_sites = len(site_dirs)\ntotal_samples_union = len(df_manifest)\ntotal_valid = int((df_manifest[\"status\"] == \"valid\").sum())\n\nmissing_rgb = int((df_manifest[\"rgb_path\"] == \"\").sum())\nmissing_depth = int((df_manifest[\"depth_path\"] == \"\").sum())\nmissing_mask = int((df_manifest[\"mask_path\"] == \"\").sum())\n\nunreadable_rgb = int(df_manifest[\"notes\"].str.contains(\"RGB unreadable\", na=False).sum())\nunreadable_depth = int(df_manifest[\"notes\"].str.contains(\"Depth unreadable\", na=False).sum())\nunreadable_mask = int(df_manifest[\"notes\"].str.contains(\"Mask unreadable\", na=False).sum())\n\nobj_mean = float(np.mean(global_object_areas)) if len(global_object_areas) else np.nan\nobj_median = float(np.median(global_object_areas)) if len(global_object_areas) else np.nan\nobj_p90 = safe_percentile(global_object_areas, 90)\nobj_p95 = safe_percentile(global_object_areas, 95)\nobj_p99 = safe_percentile(global_object_areas, 99)\n\ndepth_valid_mean = float(np.mean(global_depth_valid_ratios)) if len(global_depth_valid_ratios) else np.nan\ndepth_valid_median = float(np.median(global_depth_valid_ratios)) if len(global_depth_valid_ratios) else np.nan\n\ndef shape_to_hw(shape_str_or_tuple):\n    if shape_str_or_tuple is None or shape_str_or_tuple == \"\":\n        return None\n    if isinstance(shape_str_or_tuple, tuple):\n        s = shape_str_or_tuple\n    else:\n        m = re.findall(r\"\\d+\", str(shape_str_or_tuple))\n        if len(m) < 2:\n            return None\n        s = tuple(int(v) for v in m)\n    if len(s) >= 2:\n        return int(s[0]), int(s[1])\n    return None\n\nrgb_hw = [shape_to_hw(s) for s in global_rgb_shapes if s is not None]\ndepth_hw = [shape_to_hw(s) for s in global_depth_shapes if s is not None]\nrgb_hw = [x for x in rgb_hw if x is not None]\ndepth_hw = [x for x in depth_hw if x is not None]\n\nrgb_res_counter = Counter(rgb_hw)\ndepth_res_counter = Counter(depth_hw)\nrgb_bit_counter = Counter([b for b in global_rgb_bitdepths if b is not None])\ndepth_bit_counter = Counter([b for b in global_depth_bitdepths if b is not None])\n\nsummary_rows = [\n    (\"data_root\", str(DATA_ROOT)),\n    (\"sites_count\", total_sites),\n    (\"samples_total_union\", total_samples_union),\n    (\"valid_triplets\", total_valid),\n    (\"missing_rgb\", missing_rgb),\n    (\"missing_depth\", missing_depth),\n    (\"missing_mask\", missing_mask),\n    (\"unreadable_rgb\", unreadable_rgb),\n    (\"unreadable_depth\", unreadable_depth),\n    (\"unreadable_mask\", unreadable_mask),\n    (\"global_mean_object_area_px\", obj_mean),\n    (\"global_median_object_area_px\", obj_median),\n    (\"global_p90_object_area_px\", obj_p90),\n    (\"global_p95_object_area_px\", obj_p95),\n    (\"global_p99_object_area_px\", obj_p99),\n    (\"global_mean_depth_valid_ratio\", depth_valid_mean),\n    (\"global_median_depth_valid_ratio\", depth_valid_median),\n    (\"label_parse_unique_classes\", len(label_class_counter)),\n    (\"label_parse_total_labeled\", int(sum(label_class_counter.values()))),\n]\n\ndf_summary = pd.DataFrame(summary_rows, columns=[\"metric\", \"value\"])\nTAB_02_SUMMARY = RESULTS_DIR / \"tab_02_integrity_and_profile_summary.csv\"\ndf_summary.to_csv(TAB_02_SUMMARY, index=False)\n\nsite_profile_rows = []\nfor site_id in sorted([p.name for p in site_dirs]):\n    areas = site_object_areas.get(site_id, [])\n    dvrs = site_depth_valid_ratios.get(site_id, [])\n    site_profile_rows.append({\n        \"site_id\": site_id,\n        \"num_samples\": int(site_valid_counts.get(site_id, 0)),\n        \"mean_object_area_px\": float(np.mean(areas)) if len(areas) else np.nan,\n        \"median_object_area_px\": float(np.median(areas)) if len(areas) else np.nan,\n        \"p90_object_area_px\": safe_percentile(areas, 90),\n        \"mean_depth_valid_ratio\": float(np.mean(dvrs)) if len(dvrs) else np.nan\n    })\n\ndf_site = pd.DataFrame(site_profile_rows).sort_values(\"site_id\", kind=\"mergesort\").reset_index(drop=True)\nTAB_03_SITE = RESULTS_DIR / \"tab_03_site_profile_table.csv\"\ndf_site.to_csv(TAB_03_SITE, index=False)\n\ncounts = {\n    \"valid\": total_valid,\n    \"missing_rgb\": missing_rgb,\n    \"missing_depth\": missing_depth,\n    \"missing_mask\": missing_mask,\n    \"unreadable_rgb\": unreadable_rgb,\n    \"unreadable_depth\": unreadable_depth,\n    \"unreadable_mask\": unreadable_mask,\n}\nlabels = list(counts.keys())\nvals = [counts[k] for k in labels]\n\nplt.figure(figsize=(9, 4.5))\nplt.bar(labels, vals)\nplt.title(\"ISOD file health overview\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=25, ha=\"right\")\nmx = max(vals) if len(vals) else 0\nfor i, v in enumerate(vals):\n    plt.text(i, v + max(1, 0.01 * mx), str(v), ha=\"center\", va=\"bottom\")\nsave_fig(RESULTS_DIR / \"fig_01_file_health_overview.png\", dpi=300)\n\ndef counter_to_sorted_xy(counter: Counter):\n    items = sorted(counter.items(), key=lambda kv: (kv[0][0], kv[0][1]) if isinstance(kv[0], tuple) else kv[0])\n    xs = [str(k) for k, _ in items]\n    ys = [v for _, v in items]\n    return xs, ys\n\nrgb_res_x, rgb_res_y = counter_to_sorted_xy(rgb_res_counter)\ndepth_res_x, depth_res_y = counter_to_sorted_xy(depth_res_counter)\n\nrgb_bit_items = sorted(rgb_bit_counter.items(), key=lambda kv: kv[0])\ndepth_bit_items = sorted(depth_bit_counter.items(), key=lambda kv: kv[0])\n\nplt.figure(figsize=(11, 7))\nax1 = plt.subplot(2, 2, 1)\nax1.bar(range(len(rgb_res_x)), rgb_res_y)\nax1.set_title(\"RGB resolution\")\nax1.set_ylabel(\"Count\")\nax1.set_xticks(range(len(rgb_res_x)))\nax1.set_xticklabels(rgb_res_x, rotation=45, ha=\"right\")\n\nax2 = plt.subplot(2, 2, 2)\nax2.bar(range(len(depth_res_x)), depth_res_y)\nax2.set_title(\"Depth resolution\")\nax2.set_ylabel(\"Count\")\nax2.set_xticks(range(len(depth_res_x)))\nax2.set_xticklabels(depth_res_x, rotation=45, ha=\"right\")\n\nax3 = plt.subplot(2, 2, 3)\nax3.bar([str(k) for k, _ in rgb_bit_items], [v for _, v in rgb_bit_items])\nax3.set_title(\"RGB bit depth\")\nax3.set_ylabel(\"Count\")\nax3.set_xlabel(\"Bit depth\")\n\nax4 = plt.subplot(2, 2, 4)\nax4.bar([str(k) for k, _ in depth_bit_items], [v for _, v in depth_bit_items])\nax4.set_title(\"Depth bit depth\")\nax4.set_ylabel(\"Count\")\nax4.set_xlabel(\"Bit depth\")\n\nsave_fig(RESULTS_DIR / \"fig_02_resolution_and_bitdepth_profile.png\", dpi=300)\n\nsites_sorted = sorted(site_valid_counts.keys())\nsite_counts = [int(site_valid_counts[s]) for s in sites_sorted]\n\nplt.figure(figsize=(12, 4.8))\nplt.bar(sites_sorted, site_counts)\nplt.title(\"Valid triplet samples per site\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45, ha=\"right\")\nsave_fig(RESULTS_DIR / \"fig_03_samples_per_site.png\", dpi=300)\n\nareas = np.array(global_object_areas, dtype=np.float64) if len(global_object_areas) else np.array([])\nplt.figure(figsize=(9, 4.8))\nif len(areas):\n    a_min = max(1.0, float(np.min(areas)))\n    a_max = float(np.max(areas))\n    if a_max / a_min > 200:\n        bins = np.logspace(np.log10(a_min), np.log10(a_max), 40)\n        plt.hist(areas, bins=bins)\n        plt.xscale(\"log\")\n        plt.xlabel(\"Object area in pixels (log scale)\")\n    else:\n        plt.hist(areas, bins=40)\n        plt.xlabel(\"Object area in pixels\")\n    p50 = np.percentile(areas, 50)\n    p90 = np.percentile(areas, 90)\n    p95 = np.percentile(areas, 95)\n    plt.axvline(p50, linestyle=\"--\", linewidth=1)\n    plt.axvline(p90, linestyle=\"--\", linewidth=1)\n    plt.axvline(p95, linestyle=\"--\", linewidth=1)\n    plt.title(\"Mask object area distribution\")\n    plt.ylabel(\"Count\")\n    plt.legend([\"p50\", \"p90\", \"p95\"])\nelse:\n    plt.title(\"Mask object area distribution\")\n    plt.text(0.5, 0.5, \"No valid mask areas found\", ha=\"center\", va=\"center\")\n    plt.axis(\"off\")\n\nsave_fig(RESULTS_DIR / \"fig_04_object_size_distribution.png\", dpi=300)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nfrom pathlib import Path\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport cv2\n\nWORK_ROOT = Path(\"/kaggle/working\")\nPHASE1_DIR = WORK_ROOT / \"results\" / \"01_data_integrity_and_profile\"\nPHASE2_DIR = WORK_ROOT / \"results\" / \"02_sensor_alignment_and_quality\"\nPHASE2_DIR.mkdir(parents=True, exist_ok=True)\n\nMANIFEST_VALID_PATH = PHASE1_DIR / \"tab_01_dataset_manifest_valid_only.csv\"\nif not MANIFEST_VALID_PATH.exists():\n    raise FileNotFoundError(f\"Missing Phase 1 valid manifest: {MANIFEST_VALID_PATH}\")\n\ndf_valid = pd.read_csv(MANIFEST_VALID_PATH)\ndf_valid = df_valid.sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\n\nplt.rcParams.update({\n    \"font.size\": 10,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 10,\n    \"xtick.labelsize\": 9,\n    \"ytick.labelsize\": 9,\n    \"legend.fontsize\": 9,\n})\n\ndef save_fig(path: Path, dpi=300):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    plt.tight_layout()\n    plt.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n    plt.close()\n\ndef read_img_rgb(path: str):\n    img = Image.open(path)\n    img.load()\n    arr = np.array(img)\n    if arr.ndim == 2:\n        arr = np.stack([arr, arr, arr], axis=2)\n    if arr.shape[2] > 3:\n        arr = arr[:, :, :3]\n    return arr\n\ndef read_img_depth(path: str):\n    img = Image.open(path)\n    img.load()\n    arr = np.array(img)\n    if arr.ndim == 3:\n        arr = arr[:, :, 0]\n    return arr\n\ndef read_img_mask(path: str):\n    img = Image.open(path)\n    img.load()\n    arr = np.array(img)\n    if arr.ndim == 3:\n        arr = np.any(arr > 0, axis=2).astype(np.uint8)\n    else:\n        arr = (arr > 0).astype(np.uint8)\n    return arr\n\ndef mask_boundary(mask01: np.ndarray):\n    k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n    b = cv2.morphologyEx(mask01.astype(np.uint8), cv2.MORPH_GRADIENT, k)\n    return (b > 0).astype(np.uint8)\n\ndef canny_edges(rgb: np.ndarray):\n    gray = cv2.cvtColor(rgb.astype(np.uint8), cv2.COLOR_RGB2GRAY)\n    v = np.median(gray)\n    lo = int(max(0, 0.66 * v))\n    hi = int(min(255, 1.33 * v))\n    e = cv2.Canny(gray, lo, hi, L2gradient=True)\n    return (e > 0).astype(np.uint8)\n\ndef depth_grad_edges(depth: np.ndarray):\n    d = depth.astype(np.float32)\n    valid = d > 0\n    if np.count_nonzero(valid) < 10:\n        return np.zeros_like(d, dtype=np.uint8)\n    vv = d[valid]\n    mn, mx = float(np.min(vv)), float(np.max(vv))\n    if mx - mn < 1e-6:\n        dn = np.zeros_like(d, dtype=np.float32)\n    else:\n        dn = (d - mn) / (mx - mn)\n        dn[~valid] = 0.0\n    gx = cv2.Sobel(dn, cv2.CV_32F, 1, 0, ksize=3)\n    gy = cv2.Sobel(dn, cv2.CV_32F, 0, 1, ksize=3)\n    gm = cv2.magnitude(gx, gy)\n    gm_valid = gm[valid]\n    if gm_valid.size < 10:\n        return np.zeros_like(d, dtype=np.uint8)\n    thr = float(np.percentile(gm_valid, 90))\n    ed = (gm >= thr).astype(np.uint8)\n    return ed\n\ndef dilate_binary(b01: np.ndarray, r: int):\n    if r <= 0:\n        return b01.astype(np.uint8)\n    k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2 * r + 1, 2 * r + 1))\n    return (cv2.dilate(b01.astype(np.uint8), k, iterations=1) > 0).astype(np.uint8)\n\ndef depth_valid_ratio(depth: np.ndarray):\n    d = depth\n    valid = (d > 0) & np.isfinite(d)\n    total = d.size\n    if total == 0:\n        return np.nan\n    return float(np.count_nonzero(valid)) / float(total)\n\ndef depth_hole_rate(depth: np.ndarray):\n    r = depth_valid_ratio(depth)\n    if np.isnan(r):\n        return np.nan\n    return float(1.0 - r)\n\ndef hole_components(depth: np.ndarray):\n    invalid = ((depth <= 0) | (~np.isfinite(depth))).astype(np.uint8)\n    if invalid.size == 0:\n        return 0, 0\n    num, labels = cv2.connectedComponents(invalid, connectivity=8)\n    if num <= 1:\n        return 0, 0\n    areas = np.bincount(labels.ravel())\n    areas = areas[1:]\n    return int(len(areas)), int(np.max(areas)) if len(areas) else 0\n\ndef agreement(boundary01: np.ndarray, cue01: np.ndarray, tol: int):\n    b = boundary01.astype(np.uint8)\n    c = dilate_binary(cue01, tol)\n    idx = (b > 0)\n    denom = int(np.count_nonzero(idx))\n    if denom == 0:\n        return np.nan\n    return float(np.count_nonzero(c[idx] > 0)) / float(denom)\n\ndef alignment_error(edge_agree: float, depth_agree: float):\n    if np.isnan(edge_agree) and np.isnan(depth_agree):\n        return np.nan\n    if np.isnan(edge_agree):\n        return float(1.0 - depth_agree)\n    if np.isnan(depth_agree):\n        return float(1.0 - edge_agree)\n    return float(1.0 - 0.5 * (edge_agree + depth_agree))\n\ndef parse_shape_str(s):\n    if s is None or (isinstance(s, float) and np.isnan(s)) or str(s).strip() == \"\":\n        return None\n    nums = re.findall(r\"\\d+\", str(s))\n    if len(nums) < 2:\n        return None\n    return tuple(int(x) for x in nums)\n\nTOL_PX = 3\n\nrows = []\nfor i in range(len(df_valid)):\n    sample_id = df_valid.loc[i, \"sample_id\"]\n    site_id = df_valid.loc[i, \"site_id\"]\n    rgb_path = df_valid.loc[i, \"rgb_path\"]\n    depth_path = df_valid.loc[i, \"depth_path\"]\n    mask_path = df_valid.loc[i, \"mask_path\"]\n\n    notes = []\n    try:\n        rgb = read_img_rgb(rgb_path)\n        depth = read_img_depth(depth_path)\n        mask = read_img_mask(mask_path)\n    except Exception as e:\n        rows.append({\n            \"sample_id\": sample_id,\n            \"site_id\": site_id,\n            \"alignment_error\": np.nan,\n            \"edge_agreement\": np.nan,\n            \"depth_gradient_agreement\": np.nan,\n            \"depth_valid_ratio\": np.nan,\n            \"notes\": f\"read_error:{type(e).__name__}\"\n        })\n        continue\n\n    if rgb.shape[0] != depth.shape[0] or rgb.shape[1] != depth.shape[1] or rgb.shape[0] != mask.shape[0] or rgb.shape[1] != mask.shape[1]:\n        notes.append(\"shape_mismatch\")\n        h = min(rgb.shape[0], depth.shape[0], mask.shape[0])\n        w = min(rgb.shape[1], depth.shape[1], mask.shape[1])\n        rgb = rgb[:h, :w]\n        depth = depth[:h, :w]\n        mask = mask[:h, :w]\n\n    bnd = mask_boundary(mask)\n    rgb_e = canny_edges(rgb)\n    dep_e = depth_grad_edges(depth)\n\n    edge_agree = agreement(bnd, rgb_e, TOL_PX)\n    depth_agree = agreement(bnd, dep_e, TOL_PX)\n    aerr = alignment_error(edge_agree, depth_agree)\n\n    dvr = depth_valid_ratio(depth)\n    n_holes, max_hole = hole_components(depth)\n    if n_holes > 0:\n        notes.append(f\"holes={n_holes}\")\n        notes.append(f\"max_hole_px={max_hole}\")\n\n    rows.append({\n        \"sample_id\": sample_id,\n        \"site_id\": site_id,\n        \"alignment_error\": aerr,\n        \"edge_agreement\": edge_agree,\n        \"depth_gradient_agreement\": depth_agree,\n        \"depth_valid_ratio\": dvr,\n        \"notes\": \";\".join(notes)\n    })\n\ndf_align = pd.DataFrame(rows).sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\n\nTAB_01 = PHASE2_DIR / \"tab_01_alignment_metrics_full.csv\"\ndf_align.to_csv(TAB_01, index=False)\n\ndf_site = df_align.groupby(\"site_id\", sort=True).agg(\n    num_samples=(\"sample_id\", \"count\"),\n    mean_alignment_error=(\"alignment_error\", \"mean\"),\n    std_alignment_error=(\"alignment_error\", \"std\"),\n    mean_depth_valid_ratio=(\"depth_valid_ratio\", \"mean\"),\n)\ndf_site[\"depth_hole_rate\"] = 1.0 - df_site[\"mean_depth_valid_ratio\"]\ndf_site = df_site.reset_index().sort_values(\"site_id\", kind=\"mergesort\").reset_index(drop=True)\n\nTAB_02 = PHASE2_DIR / \"tab_02_site_quality_summary.csv\"\ndf_site.to_csv(TAB_02, index=False)\n\ndf_align_clean = df_align.dropna(subset=[\"alignment_error\"]).copy()\ndf_align_clean = df_align_clean.sort_values([\"alignment_error\", \"sample_id\"], ascending=[False, True], kind=\"mergesort\").reset_index(drop=True)\n\nTOPK_OUTLIERS = 16\ndf_out = df_align_clean.head(TOPK_OUTLIERS).copy()\n\ndef likely_cause(row):\n    notes = str(row.get(\"notes\", \"\"))\n    dvr = row.get(\"depth_valid_ratio\", np.nan)\n    if isinstance(dvr, (float, np.floating)) and not np.isnan(dvr) and dvr < 0.85:\n        return \"depth_holes_or_invalid_depth\"\n    if \"shape_mismatch\" in notes:\n        return \"sensor_resolution_mismatch_or_crop\"\n    return \"sensor_misalignment_or_edge_weakness\"\n\ndf_out[\"likely_cause\"] = df_out.apply(likely_cause, axis=1)\nTAB_03 = PHASE2_DIR / \"tab_03_alignment_outliers_table.csv\"\ndf_out[[\"sample_id\", \"site_id\", \"alignment_error\", \"depth_valid_ratio\", \"likely_cause\"]].to_csv(TAB_03, index=False)\n\ndf_align_clean_sorted = df_align_clean.sort_values([\"alignment_error\", \"sample_id\"], ascending=[True, True], kind=\"mergesort\").reset_index(drop=True)\nn = len(df_align_clean_sorted)\nidx_low = list(range(0, min(4, n)))\nidx_mid = [min(n - 1, int(round((n - 1) * q))) for q in [0.45, 0.50, 0.55, 0.60]]\nidx_high = list(range(max(0, n - 4), n))\nsel_idx = idx_low + idx_mid + idx_high\nsel_idx = [i for i in sel_idx if 0 <= i < n]\nsel_idx = list(dict.fromkeys(sel_idx))\ndf_rep = df_align_clean_sorted.iloc[sel_idx].copy()\ndf_rep = df_rep.sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\n\noverlay_ids = df_rep[\"sample_id\"].tolist()\noutlier_ids = df_out[\"sample_id\"].tolist()\n\nTAB_04 = PHASE2_DIR / \"tab_04_figure_sample_ids.csv\"\ndf_figids = pd.DataFrame([\n    {\"figure_name\": \"fig_01_rgb_depth_mask_overlay_grid.png\", \"sample_id_list\": \"|\".join(overlay_ids), \"selection_rule\": \"4 best + 4 mid + 4 worst by alignment_error with deterministic ties\"},\n    {\"figure_name\": \"fig_04_alignment_outliers_visual.png\", \"sample_id_list\": \"|\".join(outlier_ids), \"selection_rule\": f\"top {TOPK_OUTLIERS} worst alignment_error with deterministic ties\"},\n])\ndf_figids.to_csv(TAB_04, index=False)\n\ndef build_lookup(df_manifest_valid: pd.DataFrame):\n    lut = {}\n    for i in range(len(df_manifest_valid)):\n        sid = df_manifest_valid.loc[i, \"sample_id\"]\n        lut[sid] = {\n            \"site_id\": df_manifest_valid.loc[i, \"site_id\"],\n            \"rgb_path\": df_manifest_valid.loc[i, \"rgb_path\"],\n            \"depth_path\": df_manifest_valid.loc[i, \"depth_path\"],\n            \"mask_path\": df_manifest_valid.loc[i, \"mask_path\"],\n        }\n    return lut\n\nlut = build_lookup(df_valid)\n\ndef draw_boundary_overlay(ax, base_img, boundary01, title, is_depth=False):\n    if is_depth:\n        ax.imshow(base_img, cmap=\"gray\")\n    else:\n        ax.imshow(base_img)\n    ax.contour(boundary01.astype(np.uint8), levels=[0.5], colors=[\"red\"], linewidths=1.0)\n    ax.set_title(title)\n    ax.axis(\"off\")\n\ndef load_triplet_by_sample_id(sample_id):\n    p = lut[sample_id]\n    rgb = read_img_rgb(p[\"rgb_path\"])\n    depth = read_img_depth(p[\"depth_path\"])\n    mask = read_img_mask(p[\"mask_path\"])\n    h = min(rgb.shape[0], depth.shape[0], mask.shape[0])\n    w = min(rgb.shape[1], depth.shape[1], mask.shape[1])\n    rgb = rgb[:h, :w]\n    depth = depth[:h, :w]\n    mask = mask[:h, :w]\n    bnd = mask_boundary(mask)\n    return p[\"site_id\"], rgb, depth, bnd\n\ngrid_ids = overlay_ids\nm = len(grid_ids)\ncols = 4\nrows_grid = int(np.ceil(m / cols))\nplt.figure(figsize=(4 * cols, 3 * rows_grid * 2))\n\nfor k, sid in enumerate(grid_ids):\n    site_id, rgb, depth, bnd = load_triplet_by_sample_id(sid)\n    r = k // cols\n    c = k % cols\n    ax1 = plt.subplot(rows_grid * 2, cols, r * 2 * cols + c + 1)\n    draw_boundary_overlay(ax1, rgb, bnd, f\"{sid}\", is_depth=False)\n    ax2 = plt.subplot(rows_grid * 2, cols, (r * 2 + 1) * cols + c + 1)\n    draw_boundary_overlay(ax2, depth, bnd, f\"depth\", is_depth=True)\n\nsave_fig(PHASE2_DIR / \"fig_01_rgb_depth_mask_overlay_grid.png\", dpi=300)\n\ndf_site_plot = df_site.sort_values([\"mean_alignment_error\", \"site_id\"], ascending=[False, True], kind=\"mergesort\").reset_index(drop=True)\n\nplt.figure(figsize=(12, 4.8))\nplt.bar(df_site_plot[\"site_id\"].tolist(), df_site_plot[\"mean_alignment_error\"].tolist())\nplt.title(\"Mean alignment error by site\")\nplt.ylabel(\"Mean alignment error\")\nplt.xticks(rotation=45, ha=\"right\")\nsave_fig(PHASE2_DIR / \"fig_02_alignment_error_by_site.png\", dpi=300)\n\nplt.figure(figsize=(12, 4.8))\nplt.bar(df_site_plot[\"site_id\"].tolist(), df_site_plot[\"mean_depth_valid_ratio\"].tolist())\nplt.title(\"Mean depth valid ratio by site\")\nplt.ylabel(\"Mean depth valid ratio\")\nplt.ylim(0, 1.0)\nplt.xticks(rotation=45, ha=\"right\")\nsave_fig(PHASE2_DIR / \"fig_03_depth_validity_by_site.png\", dpi=300)\n\nvis_ids = outlier_ids[:min(8, len(outlier_ids))]\ncols = 2\nrows_grid = int(np.ceil(len(vis_ids) / cols))\nplt.figure(figsize=(9 * cols, 4.5 * rows_grid))\n\nalign_map = {r[\"sample_id\"]: r for r in df_out.to_dict(orient=\"records\")}\n\nfor k, sid in enumerate(vis_ids):\n    site_id, rgb, depth, bnd = load_triplet_by_sample_id(sid)\n    met = align_map.get(sid, {})\n    aerr = met.get(\"alignment_error\", np.nan)\n    dvr = met.get(\"depth_valid_ratio\", np.nan)\n    cause = met.get(\"likely_cause\", \"\")\n    ax = plt.subplot(rows_grid, cols, k + 1)\n    ax.imshow(rgb)\n    ax.contour(bnd.astype(np.uint8), levels=[0.5], colors=[\"red\"], linewidths=1.0)\n    ax.set_title(f\"{sid}\\nerr={aerr:.3f} dvr={dvr:.3f}\\n{cause}\")\n    ax.axis(\"off\")\n\nsave_fig(PHASE2_DIR / \"fig_04_alignment_outliers_visual.png\", dpi=300)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport math\nimport re\nfrom pathlib import Path\nfrom collections import Counter, defaultdict\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport cv2\n\nWORK_ROOT = Path(\"/kaggle/working\")\nPHASE1_DIR = WORK_ROOT / \"results\" / \"01_data_integrity_and_profile\"\nPHASE2_DIR = WORK_ROOT / \"results\" / \"02_sensor_alignment_and_quality\"\nPHASE3_DIR = WORK_ROOT / \"results\" / \"03_preprocess_and_split_protocol\"\nPHASE3_DIR.mkdir(parents=True, exist_ok=True)\n\nMANIFEST_VALID_PATH = PHASE1_DIR / \"tab_01_dataset_manifest_valid_only.csv\"\nALIGNMENT_PATH = PHASE2_DIR / \"tab_01_alignment_metrics_full.csv\"\n\nif not MANIFEST_VALID_PATH.exists():\n    raise FileNotFoundError(f\"Missing Phase 1 valid manifest: {MANIFEST_VALID_PATH}\")\nif not ALIGNMENT_PATH.exists():\n    raise FileNotFoundError(f\"Missing Phase 2 alignment table: {ALIGNMENT_PATH}\")\n\ndf_valid = pd.read_csv(MANIFEST_VALID_PATH).sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\ndf_align = pd.read_csv(ALIGNMENT_PATH).sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\n\ndf = df_valid.merge(df_align[[\"sample_id\", \"depth_valid_ratio\", \"notes\"]], on=\"sample_id\", how=\"left\", suffixes=(\"\", \"_align\"))\ndf[\"site_id\"] = df[\"site_id\"].astype(str)\ndf[\"sample_id\"] = df[\"sample_id\"].astype(str)\n\nplt.rcParams.update({\n    \"font.size\": 10,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 10,\n    \"xtick.labelsize\": 9,\n    \"ytick.labelsize\": 9,\n    \"legend.fontsize\": 9,\n})\n\ndef save_fig(path: Path, dpi=300):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    plt.tight_layout()\n    plt.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n    plt.close()\n\ndef read_rgb(path: str):\n    img = Image.open(path)\n    img.load()\n    arr = np.array(img)\n    if arr.ndim == 2:\n        arr = np.stack([arr, arr, arr], axis=2)\n    if arr.shape[2] > 3:\n        arr = arr[:, :, :3]\n    return arr.astype(np.uint8)\n\ndef read_depth(path: str):\n    img = Image.open(path)\n    img.load()\n    arr = np.array(img)\n    if arr.ndim == 3:\n        arr = arr[:, :, 0]\n    return arr\n\ndef read_mask(path: str):\n    img = Image.open(path)\n    img.load()\n    arr = np.array(img)\n    if arr.ndim == 3:\n        arr = np.any(arr > 0, axis=2).astype(np.uint8)\n    else:\n        arr = (arr > 0).astype(np.uint8)\n    return arr\n\ndef mask_area(mask01: np.ndarray):\n    return int(np.count_nonzero(mask01 > 0))\n\ndef depth_valid_ratio(depth: np.ndarray):\n    d = depth\n    valid = (d > 0) & np.isfinite(d)\n    total = d.size\n    if total == 0:\n        return np.nan\n    return float(np.count_nonzero(valid)) / float(total)\n\ndef fill_depth_holes(depth: np.ndarray):\n    d = depth.astype(np.float32)\n    valid = (d > 0) & np.isfinite(d)\n    if np.count_nonzero(valid) < 10:\n        return d, \"too_few_valid\"\n    invalid = (~valid).astype(np.uint8)\n    d0 = d.copy()\n    d0[~valid] = 0.0\n    filled = cv2.inpaint(d0, invalid, 3, cv2.INPAINT_TELEA)\n    filled[~np.isfinite(filled)] = 0.0\n    return filled, \"\"\n\ndef depth_to_uint8_for_viz(depth: np.ndarray):\n    d = depth.astype(np.float32)\n    valid = (d > 0) & np.isfinite(d)\n    if np.count_nonzero(valid) < 10:\n        return np.zeros_like(d, dtype=np.uint8)\n    vv = d[valid]\n    mn, mx = float(np.percentile(vv, 1)), float(np.percentile(vv, 99))\n    if mx - mn < 1e-6:\n        return np.zeros_like(d, dtype=np.uint8)\n    dn = (d - mn) / (mx - mn)\n    dn = np.clip(dn, 0.0, 1.0)\n    return (dn * 255.0).astype(np.uint8)\n\ndef resize_triplet(rgb: np.ndarray, depth: np.ndarray, mask01: np.ndarray, out_hw):\n    oh, ow = int(out_hw[0]), int(out_hw[1])\n    rh, rw = rgb.shape[0], rgb.shape[1]\n    if (rh == oh) and (rw == ow):\n        return rgb, depth, mask01, {\"resize_mode\": \"none\", \"scale\": 1.0}\n    rgb_r = cv2.resize(rgb, (ow, oh), interpolation=cv2.INTER_LINEAR)\n    if depth.dtype.kind in [\"u\", \"i\"] and depth.dtype.itemsize >= 2:\n        depth_f = depth.astype(np.float32)\n        depth_r = cv2.resize(depth_f, (ow, oh), interpolation=cv2.INTER_NEAREST)\n    else:\n        depth_f = depth.astype(np.float32)\n        depth_r = cv2.resize(depth_f, (ow, oh), interpolation=cv2.INTER_NEAREST)\n    mask_r = cv2.resize(mask01.astype(np.uint8), (ow, oh), interpolation=cv2.INTER_NEAREST)\n    scale = float(oh) / float(rh) if rh > 0 else np.nan\n    return rgb_r, depth_r, (mask_r > 0).astype(np.uint8), {\"resize_mode\": \"resize\", \"scale\": scale}\n\ndef choose_target_hw(df_in: pd.DataFrame):\n    hs = []\n    ws = []\n    for s in df_in[\"rgb_shape\"].astype(str).tolist():\n        nums = re.findall(r\"\\d+\", s)\n        if len(nums) >= 2:\n            hs.append(int(nums[0]))\n            ws.append(int(nums[1]))\n    if len(hs) == 0:\n        return (480, 640)\n    hw = Counter(list(zip(hs, ws)))\n    target = sorted(hw.items(), key=lambda kv: (-kv[1], kv[0][0], kv[0][1]))[0][0]\n    return (int(target[0]), int(target[1]))\n\nTARGET_HW = choose_target_hw(df_valid)\n\nPREPROCESS_CONFIG = [\n    {\"step\": \"load\", \"modality\": \"rgb\", \"operation\": \"PIL_load_to_numpy_uint8\", \"parameters\": \"keep_first_3_channels\", \"rationale\": \"standardize channel layout\"},\n    {\"step\": \"load\", \"modality\": \"depth\", \"operation\": \"PIL_load_to_numpy\", \"parameters\": \"use_first_channel_if_multichannel\", \"rationale\": \"standardize depth as single channel\"},\n    {\"step\": \"load\", \"modality\": \"mask\", \"operation\": \"PIL_load_to_binary_uint8\", \"parameters\": \"mask=any(channel>0)\", \"rationale\": \"standardize mask as binary\"},\n    {\"step\": \"align_shape\", \"modality\": \"all\", \"operation\": \"crop_to_min_common_hw\", \"parameters\": \"h=min(h_rgb,h_depth,h_mask), w=min(w_rgb,w_depth,w_mask)\", \"rationale\": \"avoid shape mismatch failures\"},\n    {\"step\": \"resize\", \"modality\": \"all\", \"operation\": \"resize_to_target_hw\", \"parameters\": f\"target_hw={TARGET_HW}, rgb=bilinear, depth=nearest, mask=nearest\", \"rationale\": \"consistent input size for all models\"},\n    {\"step\": \"depth_clean\", \"modality\": \"depth\", \"operation\": \"inpaint_invalid_depth\", \"parameters\": \"invalid=(depth<=0 or nonfinite), method=TELEA, radius=3\", \"rationale\": \"reduce holes and stabilize gradients\"},\n    {\"step\": \"depth_scale\", \"modality\": \"depth\", \"operation\": \"robust_minmax_for_visualization\", \"parameters\": \"viz only: p1-p99 to uint8\", \"rationale\": \"paper figures and debugging\"},\n]\n\nTAB_01 = PHASE3_DIR / \"tab_01_preprocess_config_table.csv\"\npd.DataFrame(PREPROCESS_CONFIG)[[\"step\", \"modality\", \"operation\", \"parameters\", \"rationale\"]].to_csv(TAB_01, index=False)\n\nstats_rows = []\nexample_rows = []\n\nN_EXAMPLES = 8\nEXAMPLE_SAMPLE_IDS = df.sort_values(\"sample_id\", kind=\"mergesort\").head(N_EXAMPLES)[\"sample_id\"].tolist()\n\nfor i in range(len(df)):\n    sample_id = df.loc[i, \"sample_id\"]\n    site_id = df.loc[i, \"site_id\"]\n    rgb_path = df.loc[i, \"rgb_path\"]\n    depth_path = df.loc[i, \"depth_path\"]\n    mask_path = df.loc[i, \"mask_path\"]\n\n    try:\n        rgb0 = read_rgb(rgb_path)\n        depth0 = read_depth(depth_path)\n        mask0 = read_mask(mask_path)\n    except Exception as e:\n        stats_rows.append({\n            \"sample_id\": sample_id,\n            \"site_id\": site_id,\n            \"depth_valid_ratio_before\": np.nan,\n            \"depth_valid_ratio_after\": np.nan,\n            \"mask_area_px_before\": np.nan,\n            \"mask_area_px_after\": np.nan,\n            \"notes\": f\"read_error:{type(e).__name__}\"\n        })\n        continue\n\n    h = min(rgb0.shape[0], depth0.shape[0], mask0.shape[0])\n    w = min(rgb0.shape[1], depth0.shape[1], mask0.shape[1])\n    rgb0 = rgb0[:h, :w]\n    depth0 = depth0[:h, :w]\n    mask0 = mask0[:h, :w]\n\n    dvr_before = depth_valid_ratio(depth0)\n    area_before = mask_area(mask0)\n\n    rgb1, depth1, mask1, rz_info = resize_triplet(rgb0, depth0, mask0, TARGET_HW)\n\n    depth2, note_fill = fill_depth_holes(depth1)\n    dvr_after = depth_valid_ratio(depth2)\n    area_after = mask_area(mask1)\n\n    notes = []\n    if note_fill:\n        notes.append(note_fill)\n    if isinstance(rz_info, dict) and rz_info.get(\"resize_mode\", \"\") != \"none\":\n        notes.append(f\"resized_to={TARGET_HW}\")\n\n    stats_rows.append({\n        \"sample_id\": sample_id,\n        \"site_id\": site_id,\n        \"depth_valid_ratio_before\": dvr_before,\n        \"depth_valid_ratio_after\": dvr_after,\n        \"mask_area_px_before\": area_before,\n        \"mask_area_px_after\": area_after,\n        \"notes\": \";\".join(notes)\n    })\n\n    if sample_id in EXAMPLE_SAMPLE_IDS:\n        example_rows.append((sample_id, site_id, rgb0, depth0, mask0, rgb1, depth1, mask1, depth2))\n\ndf_stats = pd.DataFrame(stats_rows).sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\nTAB_02 = PHASE3_DIR / \"tab_02_preprocess_stats_full.csv\"\ndf_stats.to_csv(TAB_02, index=False)\n\nSPLIT_SEED = 1337\nTRAIN_FRAC = 0.70\nVAL_FRAC = 0.15\nTEST_FRAC = 0.15\n\nsites = sorted(df[\"site_id\"].unique().tolist())\nrng = np.random.default_rng(SPLIT_SEED)\nsites_shuffled = sites.copy()\nrng.shuffle(sites_shuffled)\n\nn_sites = len(sites_shuffled)\nn_train = int(round(TRAIN_FRAC * n_sites))\nn_val = int(round(VAL_FRAC * n_sites))\nn_test = n_sites - n_train - n_val\nif n_test < 1:\n    n_test = 1\n    if n_val > 1:\n        n_val -= 1\n    else:\n        n_train -= 1\n\ntrain_sites = sorted(sites_shuffled[:n_train])\nval_sites = sorted(sites_shuffled[n_train:n_train + n_val])\ntest_sites = sorted(sites_shuffled[n_train + n_val:])\n\nsite_to_split = {}\nfor s in train_sites:\n    site_to_split[s] = \"train\"\nfor s in val_sites:\n    site_to_split[s] = \"val\"\nfor s in test_sites:\n    site_to_split[s] = \"test\"\n\ndf_split = df[[\"sample_id\", \"site_id\"]].copy()\ndf_split[\"split\"] = df_split[\"site_id\"].map(site_to_split).astype(str)\ndf_split = df_split.sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\n\nTAB_03 = PHASE3_DIR / \"tab_03_split_manifest.csv\"\ndf_split.to_csv(TAB_03, index=False)\n\ndf_stats_m = df_stats.merge(df_split, on=[\"sample_id\", \"site_id\"], how=\"left\")\n\nPROTOCOL = [\n    {\"item\": \"leakage_control\", \"value\": \"site_disjoint_splits\"},\n    {\"item\": \"split_seed\", \"value\": str(SPLIT_SEED)},\n    {\"item\": \"split_fractions_by_site\", \"value\": f\"train={TRAIN_FRAC}, val={VAL_FRAC}, test={TEST_FRAC}\"},\n    {\"item\": \"target_hw\", \"value\": str(TARGET_HW)},\n    {\"item\": \"mask_threshold\", \"value\": \"mask_binary_nonzero\"},\n    {\"item\": \"depth_invalid_definition\", \"value\": \"depth<=0 or nonfinite\"},\n    {\"item\": \"depth_hole_fill\", \"value\": \"inpaint_TELEA_radius3\"},\n    {\"item\": \"alignment_metric_reference\", \"value\": \"Phase2 alignment_error with tol_px=3\"},\n    {\"item\": \"primary_metrics\", \"value\": \"IoU, Dice, Boundary_F1\"},\n    {\"item\": \"aux_metrics\", \"value\": \"MAE_depth_valid_ratio, outlier_rate\"},\n    {\"item\": \"postprocessing\", \"value\": \"keep_largest_component optional, threshold=0.5\"},\n    {\"item\": \"threshold_policy\", \"value\": \"fixed 0.5 for masks, report sensitivity in ablation\"},\n]\n\nTAB_04 = PHASE3_DIR / \"tab_04_protocol_metrics_table.csv\"\npd.DataFrame(PROTOCOL)[[\"item\", \"value\"]].to_csv(TAB_04, index=False)\n\nex = example_rows\ncols = 4\nrows_grid = int(math.ceil(len(ex) / cols))\nplt.figure(figsize=(4 * cols, 3.2 * rows_grid * 2))\n\nfor k, (sample_id, site_id, rgb0, depth0, mask0, rgb1, depth1, mask1, depth2) in enumerate(ex):\n    r = k // cols\n    c = k % cols\n    ax1 = plt.subplot(rows_grid * 2, cols, r * 2 * cols + c + 1)\n    rgb0v = rgb0.copy()\n    b0 = (cv2.morphologyEx(mask0.astype(np.uint8), cv2.MORPH_GRADIENT, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))) > 0).astype(np.uint8)\n    ax1.imshow(rgb0v)\n    ax1.contour(b0, levels=[0.5], colors=[\"red\"], linewidths=1.0)\n    ax1.set_title(f\"before {sample_id}\")\n    ax1.axis(\"off\")\n    ax2 = plt.subplot(rows_grid * 2, cols, (r * 2 + 1) * cols + c + 1)\n    rgb1v = rgb1.copy()\n    b1 = (cv2.morphologyEx(mask1.astype(np.uint8), cv2.MORPH_GRADIENT, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))) > 0).astype(np.uint8)\n    ax2.imshow(rgb1v)\n    ax2.contour(b1, levels=[0.5], colors=[\"red\"], linewidths=1.0)\n    ax2.set_title(\"after\")\n    ax2.axis(\"off\")\n\nsave_fig(PHASE3_DIR / \"fig_01_preprocessing_before_after_examples.png\", dpi=300)\n\ndepth_before_vals = []\ndepth_after_vals = []\n\nsample_for_hist = df.sort_values(\"sample_id\", kind=\"mergesort\").head(300)[\"sample_id\"].tolist()\nlut = {df.loc[i, \"sample_id\"]: (df.loc[i, \"depth_path\"]) for i in range(len(df))}\n\nfor sid in sample_for_hist:\n    dp = lut[sid]\n    try:\n        d0 = read_depth(dp)\n        dvr0 = depth_valid_ratio(d0)\n        if not np.isnan(dvr0):\n            valid0 = (d0 > 0) & np.isfinite(d0)\n            if np.count_nonzero(valid0) > 0:\n                vv0 = d0[valid0].astype(np.float32)\n                depth_before_vals.append(vv0)\n        d1, _ = fill_depth_holes(d0)\n        valid1 = (d1 > 0) & np.isfinite(d1)\n        if np.count_nonzero(valid1) > 0:\n            vv1 = d1[valid1].astype(np.float32)\n            depth_after_vals.append(vv1)\n    except Exception:\n        continue\n\nif len(depth_before_vals):\n    depth_before_vals = np.concatenate(depth_before_vals)\nelse:\n    depth_before_vals = np.array([], dtype=np.float32)\nif len(depth_after_vals):\n    depth_after_vals = np.concatenate(depth_after_vals)\nelse:\n    depth_after_vals = np.array([], dtype=np.float32)\n\nplt.figure(figsize=(9, 4.8))\nif depth_before_vals.size and depth_after_vals.size:\n    p1b, p99b = np.percentile(depth_before_vals, [1, 99])\n    p1a, p99a = np.percentile(depth_after_vals, [1, 99])\n    lo = float(min(p1b, p1a))\n    hi = float(max(p99b, p99a))\n    bins = 60\n    plt.hist(depth_before_vals, bins=bins, range=(lo, hi), alpha=0.6, label=\"before\")\n    plt.hist(depth_after_vals, bins=bins, range=(lo, hi), alpha=0.6, label=\"after\")\n    plt.title(\"Depth distribution before and after cleaning\")\n    plt.xlabel(\"Depth value\")\n    plt.ylabel(\"Count\")\n    plt.legend()\nelse:\n    plt.title(\"Depth distribution before and after cleaning\")\n    plt.text(0.5, 0.5, \"Insufficient depth values for histogram\", ha=\"center\", va=\"center\")\n    plt.axis(\"off\")\n\nsave_fig(PHASE3_DIR / \"fig_02_depth_distribution_before_after.png\", dpi=300)\n\ncounts_by_site_split = df_split.groupby([\"site_id\", \"split\"], sort=True).size().reset_index(name=\"count\")\nsite_order = sorted(df_split[\"site_id\"].unique().tolist())\nsplit_order = [\"train\", \"val\", \"test\"]\npivot = counts_by_site_split.pivot(index=\"site_id\", columns=\"split\", values=\"count\").reindex(site_order).fillna(0).reindex(columns=split_order).fillna(0)\n\nx = np.arange(len(pivot.index))\nplt.figure(figsize=(12, 4.8))\nbottom = np.zeros(len(pivot.index), dtype=np.float64)\nfor sp in split_order:\n    vals = pivot[sp].values.astype(np.float64)\n    plt.bar(x, vals, bottom=bottom, label=sp)\n    bottom += vals\nplt.title(\"Train val test sample counts by site\")\nplt.ylabel(\"Count\")\nplt.xticks(x, pivot.index.tolist(), rotation=45, ha=\"right\")\nplt.legend()\nsave_fig(PHASE3_DIR / \"fig_03_split_distribution_by_site.png\", dpi=300)\n\ndf_stats_m2 = df_stats_m.dropna(subset=[\"mask_area_px_after\"]).copy()\nplt.figure(figsize=(9, 4.8))\nbins = 60\nfor sp in [\"train\", \"val\", \"test\"]:\n    vals = df_stats_m2[df_stats_m2[\"split\"] == sp][\"mask_area_px_after\"].astype(np.float64).values\n    if vals.size:\n        plt.hist(vals, bins=bins, alpha=0.6, label=sp)\nplt.title(\"Object area distribution across splits\")\nplt.xlabel(\"Mask object area in pixels after preprocessing\")\nplt.ylabel(\"Count\")\nplt.legend()\nsave_fig(PHASE3_DIR / \"fig_04_object_size_by_split.png\", dpi=300)\n\ntrain_sites_set = set(train_sites)\nval_sites_set = set(val_sites)\ntest_sites_set = set(test_sites)\nassert len(train_sites_set & val_sites_set) == 0\nassert len(train_sites_set & test_sites_set) == 0\nassert len(val_sites_set & test_sites_set) == 0\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nimport json\nimport math\nimport random\nimport re\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom typing import Tuple, List, Dict\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\ntry:\n    import timm\n    _HAS_TIMM = True\nexcept Exception:\n    _HAS_TIMM = False\n\nWORK_ROOT = Path(\"/kaggle/working\")\nPHASE1_DIR = WORK_ROOT / \"results\" / \"01_data_integrity_and_profile\"\nPHASE2_DIR = WORK_ROOT / \"results\" / \"02_sensor_alignment_and_quality\"\nPHASE3_DIR = WORK_ROOT / \"results\" / \"03_preprocess_and_split_protocol\"\nPHASE4_DIR = WORK_ROOT / \"results\" / \"04_training_baselines_and_vit_fusion\"\nPHASE4_DIR.mkdir(parents=True, exist_ok=True)\n\nMANIFEST_VALID_PATH = PHASE1_DIR / \"tab_01_dataset_manifest_valid_only.csv\"\nSPLIT_MANIFEST_PATH = PHASE3_DIR / \"tab_03_split_manifest.csv\"\nPREPROCESS_CONFIG_PATH = PHASE3_DIR / \"tab_01_preprocess_config_table.csv\"\nFIG_SAMPLE_IDS_PATH = PHASE2_DIR / \"tab_04_figure_sample_ids.csv\"\n\nif not MANIFEST_VALID_PATH.exists():\n    raise FileNotFoundError(str(MANIFEST_VALID_PATH))\nif not SPLIT_MANIFEST_PATH.exists():\n    raise FileNotFoundError(str(SPLIT_MANIFEST_PATH))\nif not PREPROCESS_CONFIG_PATH.exists():\n    raise FileNotFoundError(str(PREPROCESS_CONFIG_PATH))\n\ndf_valid = pd.read_csv(MANIFEST_VALID_PATH).sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\ndf_split = pd.read_csv(SPLIT_MANIFEST_PATH).sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\ndf = df_valid.merge(df_split, on=[\"sample_id\", \"site_id\"], how=\"inner\")\ndf = df.sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\n\nTARGET_HW = None\ntry:\n    cfg = pd.read_csv(PREPROCESS_CONFIG_PATH)\n    for s in cfg[\"parameters\"].astype(str).tolist():\n        if \"target_hw=\" in s:\n            t = s.split(\"target_hw=\")[1].strip()\n            m = re.search(r\"\\((\\d+)\\s*,\\s*(\\d+)\\)\", t)\n            if m:\n                TARGET_HW = (int(m.group(1)), int(m.group(2)))\n                break\nexcept Exception:\n    TARGET_HW = None\nif TARGET_HW is None:\n    TARGET_HW = (422, 640)\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEED = 1337\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(SEED)\n\nplt.rcParams.update({\n    \"font.size\": 10,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 10,\n    \"xtick.labelsize\": 9,\n    \"ytick.labelsize\": 9,\n    \"legend.fontsize\": 9,\n})\n\ndef save_fig(path: Path, dpi=300):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    plt.tight_layout()\n    plt.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n    plt.close()\n\ndef read_rgb(path: str):\n    img = Image.open(path)\n    img.load()\n    arr = np.array(img)\n    if arr.ndim == 2:\n        arr = np.stack([arr, arr, arr], axis=2)\n    if arr.shape[2] > 3:\n        arr = arr[:, :, :3]\n    return arr.astype(np.uint8)\n\ndef read_depth(path: str):\n    img = Image.open(path)\n    img.load()\n    arr = np.array(img)\n    if arr.ndim == 3:\n        arr = arr[:, :, 0]\n    return arr\n\ndef read_mask(path: str):\n    img = Image.open(path)\n    img.load()\n    arr = np.array(img)\n    if arr.ndim == 3:\n        arr = np.any(arr > 0, axis=2).astype(np.uint8)\n    else:\n        arr = (arr > 0).astype(np.uint8)\n    return arr\n\ndef crop_to_min_hw(rgb, depth, mask):\n    h = min(rgb.shape[0], depth.shape[0], mask.shape[0])\n    w = min(rgb.shape[1], depth.shape[1], mask.shape[1])\n    return rgb[:h, :w], depth[:h, :w], mask[:h, :w]\n\ndef resize_triplet(rgb, depth, mask01, out_hw):\n    oh, ow = int(out_hw[0]), int(out_hw[1])\n    rgb_r = cv2.resize(rgb, (ow, oh), interpolation=cv2.INTER_LINEAR)\n    depth_f = depth.astype(np.float32)\n    depth_r = cv2.resize(depth_f, (ow, oh), interpolation=cv2.INTER_NEAREST)\n    mask_r = cv2.resize(mask01.astype(np.uint8), (ow, oh), interpolation=cv2.INTER_NEAREST)\n    mask_r = (mask_r > 0).astype(np.uint8)\n    return rgb_r, depth_r, mask_r\n\ndef fill_depth_holes(depth: np.ndarray):\n    d = depth.astype(np.float32)\n    valid = (d > 0) & np.isfinite(d)\n    if np.count_nonzero(valid) < 10:\n        return d\n    invalid = (~valid).astype(np.uint8)\n    d0 = d.copy()\n    d0[~valid] = 0.0\n    filled = cv2.inpaint(d0, invalid, 3, cv2.INPAINT_TELEA)\n    filled[~np.isfinite(filled)] = 0.0\n    return filled\n\ndef robust_depth_scale(depth: np.ndarray):\n    d = depth.astype(np.float32)\n    valid = (d > 0) & np.isfinite(d)\n    if np.count_nonzero(valid) < 10:\n        return np.zeros_like(d, dtype=np.float32)\n    vv = d[valid]\n    mn, mx = float(np.percentile(vv, 1)), float(np.percentile(vv, 99))\n    if mx - mn < 1e-6:\n        out = np.zeros_like(d, dtype=np.float32)\n        out[valid] = 0.5\n        return out\n    out = (d - mn) / (mx - mn)\n    out = np.clip(out, 0.0, 1.0)\n    out[~valid] = 0.0\n    return out.astype(np.float32)\n\ndef augment_pair(rgb_u8, depth_f, mask_u8, rng: np.random.Generator):\n    if rng.random() < 0.5:\n        rgb_u8 = np.ascontiguousarray(rgb_u8[:, ::-1])\n        depth_f = np.ascontiguousarray(depth_f[:, ::-1])\n        mask_u8 = np.ascontiguousarray(mask_u8[:, ::-1])\n    if rng.random() < 0.25:\n        factor = 0.8 + 0.4 * rng.random()\n        rgb_f = rgb_u8.astype(np.float32) * factor\n        rgb_u8 = np.clip(rgb_f, 0, 255).astype(np.uint8)\n    if rng.random() < 0.15:\n        noise = rng.normal(0, 3.0, size=rgb_u8.shape).astype(np.float32)\n        rgb_u8 = np.clip(rgb_u8.astype(np.float32) + noise, 0, 255).astype(np.uint8)\n    return rgb_u8, depth_f, mask_u8\n\nclass ISODSegDataset(Dataset):\n    def __init__(self, df_in: pd.DataFrame, split: str, target_hw: Tuple[int,int], seed: int, use_depth: bool):\n        self.df = df_in[df_in[\"split\"] == split].sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\n        self.target_hw = target_hw\n        self.use_depth = use_depth\n        self.rng = np.random.default_rng(seed + (0 if split == \"train\" else 999))\n        self.split = split\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.loc[idx]\n        rgb = read_rgb(row[\"rgb_path\"])\n        depth = read_depth(row[\"depth_path\"])\n        mask = read_mask(row[\"mask_path\"])\n        rgb, depth, mask = crop_to_min_hw(rgb, depth, mask)\n        rgb, depth, mask = resize_triplet(rgb, depth, mask, self.target_hw)\n        depth = fill_depth_holes(depth)\n        depth_scaled = robust_depth_scale(depth)\n        if self.split == \"train\":\n            rgb, depth_scaled, mask = augment_pair(rgb, depth_scaled, mask, self.rng)\n        rgb_t = torch.from_numpy(rgb.astype(np.float32) / 255.0).permute(2,0,1)\n        mask_t = torch.from_numpy(mask.astype(np.float32)).unsqueeze(0)\n        dep_t = torch.from_numpy(depth_scaled.astype(np.float32)).unsqueeze(0)\n        if self.use_depth:\n            x = torch.cat([rgb_t, dep_t], dim=0)\n        else:\n            x = rgb_t\n        return x, mask_t, str(row[\"sample_id\"])\n\ndef sigmoid_thresh(x, thr=0.5):\n    return (torch.sigmoid(x) >= thr).float()\n\ndef dice_coeff(pred01, gt01, eps=1e-6):\n    inter = (pred01 * gt01).sum(dim=(1,2,3))\n    denom = pred01.sum(dim=(1,2,3)) + gt01.sum(dim=(1,2,3))\n    return ((2.0 * inter + eps) / (denom + eps)).mean().item()\n\ndef iou_score(pred01, gt01, eps=1e-6):\n    inter = (pred01 * gt01).sum(dim=(1,2,3))\n    union = ((pred01 + gt01) > 0).float().sum(dim=(1,2,3))\n    return ((inter + eps) / (union + eps)).mean().item()\n\nclass DiceLoss(nn.Module):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n    def forward(self, logits, targets):\n        probs = torch.sigmoid(logits)\n        num = 2.0 * (probs * targets).sum(dim=(1,2,3)) + self.eps\n        den = probs.sum(dim=(1,2,3)) + targets.sum(dim=(1,2,3)) + self.eps\n        return (1.0 - (num / den)).mean()\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        )\n    def forward(self, x):\n        return self.net(x)\n\ndef center_crop_or_pad_to(x, ref):\n    _, _, h, w = x.shape\n    _, _, hr, wr = ref.shape\n    if h == hr and w == wr:\n        return x\n    if h > hr or w > wr:\n        dh = max(0, h - hr)\n        dw = max(0, w - wr)\n        top = dh // 2\n        left = dw // 2\n        x = x[:, :, top:top+hr, left:left+wr]\n    _, _, h2, w2 = x.shape\n    if h2 < hr or w2 < wr:\n        ph = max(0, hr - h2)\n        pw = max(0, wr - w2)\n        pad = (pw//2, pw - pw//2, ph//2, ph - ph//2)\n        x = F.pad(x, pad, mode=\"replicate\")\n    return x\n\nclass SimpleUNet(nn.Module):\n    def __init__(self, in_ch=3, base=32):\n        super().__init__()\n        self.enc1 = ConvBlock(in_ch, base)\n        self.pool1 = nn.MaxPool2d(2, ceil_mode=True)\n        self.enc2 = ConvBlock(base, base*2)\n        self.pool2 = nn.MaxPool2d(2, ceil_mode=True)\n        self.enc3 = ConvBlock(base*2, base*4)\n        self.pool3 = nn.MaxPool2d(2, ceil_mode=True)\n        self.bott = ConvBlock(base*4, base*8)\n        self.up3 = nn.ConvTranspose2d(base*8, base*4, 2, stride=2)\n        self.dec3 = ConvBlock(base*8, base*4)\n        self.up2 = nn.ConvTranspose2d(base*4, base*2, 2, stride=2)\n        self.dec2 = ConvBlock(base*4, base*2)\n        self.up1 = nn.ConvTranspose2d(base*2, base, 2, stride=2)\n        self.dec1 = ConvBlock(base*2, base)\n        self.head = nn.Conv2d(base, 1, 1)\n\n    def forward(self, x):\n        e1 = self.enc1(x)\n        e2 = self.enc2(self.pool1(e1))\n        e3 = self.enc3(self.pool2(e2))\n        b = self.bott(self.pool3(e3))\n\n        d3 = self.up3(b)\n        e3a = center_crop_or_pad_to(e3, d3)\n        d3 = self.dec3(torch.cat([d3, e3a], dim=1))\n\n        d2 = self.up2(d3)\n        e2a = center_crop_or_pad_to(e2, d2)\n        d2 = self.dec2(torch.cat([d2, e2a], dim=1))\n\n        d1 = self.up1(d2)\n        e1a = center_crop_or_pad_to(e1, d1)\n        d1 = self.dec1(torch.cat([d1, e1a], dim=1))\n\n        out = self.head(d1)\n        out = F.interpolate(out, size=(x.shape[2], x.shape[3]), mode=\"bilinear\", align_corners=False)\n        return out\n\nclass ViTBackbone(nn.Module):\n    def __init__(self, name: str, in_chans: int):\n        super().__init__()\n        if not _HAS_TIMM:\n            raise RuntimeError(\"timm not available\")\n        self.model = timm.create_model(name, pretrained=True, in_chans=in_chans, features_only=True, out_indices=(3,))\n        self.out_ch = self.model.feature_info.channels()[-1]\n    def forward(self, x):\n        feats = self.model(x)\n        return feats[-1]\n\nclass CrossAttentionFusion(nn.Module):\n    def __init__(self, ch: int, heads: int = 8):\n        super().__init__()\n        self.q = nn.Conv2d(ch, ch, 1, bias=False)\n        self.k = nn.Conv2d(ch, ch, 1, bias=False)\n        self.v = nn.Conv2d(ch, ch, 1, bias=False)\n        self.attn = nn.MultiheadAttention(embed_dim=ch, num_heads=heads, batch_first=True)\n        self.proj = nn.Conv2d(ch, ch, 1, bias=False)\n        self.norm = nn.LayerNorm(ch)\n    def forward(self, frgb, fdep):\n        b, c, h, w = frgb.shape\n        q = self.q(frgb).flatten(2).transpose(1,2)\n        k = self.k(fdep).flatten(2).transpose(1,2)\n        v = self.v(fdep).flatten(2).transpose(1,2)\n        qn = self.norm(q)\n        out, _ = self.attn(qn, k, v, need_weights=False)\n        out = out.transpose(1,2).reshape(b, c, h, w)\n        out = self.proj(out)\n        return frgb + out\n\nclass LiteDecoder(nn.Module):\n    def __init__(self, in_ch: int, out_hw: Tuple[int,int]):\n        super().__init__()\n        self.out_hw = out_hw\n        self.conv1 = ConvBlock(in_ch, 256)\n        self.up1 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        self.conv2 = ConvBlock(128, 128)\n        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n        self.conv3 = ConvBlock(64, 64)\n        self.up3 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n        self.conv4 = ConvBlock(32, 32)\n        self.head = nn.Conv2d(32, 1, 1)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.up1(x)\n        x = self.conv2(x)\n        x = self.up2(x)\n        x = self.conv3(x)\n        x = self.up3(x)\n        x = self.conv4(x)\n        x = self.head(x)\n        x = F.interpolate(x, size=self.out_hw, mode=\"bilinear\", align_corners=False)\n        return x\n\nclass ProposedViTFusion(nn.Module):\n    def __init__(self, rgb_vit_name: str, depth_vit_name: str, img_hw: Tuple[int,int]):\n        super().__init__()\n        if not _HAS_TIMM:\n            raise RuntimeError(\"timm not available\")\n        self.rgb_enc = ViTBackbone(rgb_vit_name, in_chans=3)\n        self.dep_enc = ViTBackbone(depth_vit_name, in_chans=1)\n        ch = self.rgb_enc.out_ch\n        self.dep_proj = nn.Conv2d(self.dep_enc.out_ch, ch, 1, bias=False) if self.dep_enc.out_ch != ch else nn.Identity()\n        self.fuse = CrossAttentionFusion(ch, heads=8)\n        self.dec = LiteDecoder(ch, out_hw=img_hw)\n    def forward(self, rgb3, dep1):\n        fr = self.rgb_enc(rgb3)\n        fd = self.dep_proj(self.dep_enc(dep1))\n        f = self.fuse(fr, fd)\n        return self.dec(f)\n\ndef count_params_m(model: nn.Module):\n    return sum(p.numel() for p in model.parameters()) / 1e6\n\ndef try_flops_g(model: nn.Module, example_inputs: Tuple[torch.Tensor, ...]):\n    try:\n        from fvcore.nn import FlopCountAnalysis\n        flops = FlopCountAnalysis(model, example_inputs).total()\n        return float(flops) / 1e9\n    except Exception:\n        return np.nan\n\ndef get_gpu_name():\n    if not torch.cuda.is_available():\n        return \"cpu\"\n    try:\n        return torch.cuda.get_device_name(0)\n    except Exception:\n        return \"cuda\"\n\n@dataclass\nclass TrainConfig:\n    epochs: int\n    batch_size: int\n    lr: float\n    weight_decay: float\n    optimizer: str\n    augmentations: str\n    seed: int\n    grad_accum: int\n\ndef make_loader(split: str, batch_size: int, use_depth: bool):\n    ds = ISODSegDataset(df, split=split, target_hw=TARGET_HW, seed=SEED, use_depth=use_depth)\n    shuffle = (split == \"train\")\n    dl = DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=2, pin_memory=True, drop_last=False)\n    return ds, dl\n\ndef train_one_model(model_key: str, model: nn.Module, use_depth_input: bool, cfg: TrainConfig):\n    out_dir = PHASE4_DIR / model_key\n    out_dir.mkdir(parents=True, exist_ok=True)\n    ckpt_dir = out_dir / \"checkpoints\"\n    ckpt_dir.mkdir(parents=True, exist_ok=True)\n\n    model = model.to(DEVICE)\n\n    _, dl_tr = make_loader(\"train\", cfg.batch_size, use_depth=use_depth_input)\n    _, dl_va = make_loader(\"val\", cfg.batch_size, use_depth=use_depth_input)\n\n    if cfg.optimizer.lower() == \"adamw\":\n        opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n    else:\n        opt = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n\n    bce = nn.BCEWithLogitsLoss()\n    dice = DiceLoss()\n    scaler = torch.amp.GradScaler(\"cuda\", enabled=torch.cuda.is_available())\n\n    best_metric = -1.0\n    best_epoch = -1\n    best_path = \"\"\n\n    logs = []\n\n    start_time = time.time()\n    if torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats()\n\n    for epoch in range(1, cfg.epochs + 1):\n        model.train()\n        tr_loss_sum = 0.0\n        tr_steps = 0\n        opt.zero_grad(set_to_none=True)\n\n        for step, (x, y, sids) in enumerate(dl_tr, start=1):\n            y = y.to(DEVICE, non_blocking=True)\n\n            if model_key == \"proposed_vit_fusion\":\n                rgb = x[:, :3].to(DEVICE, non_blocking=True)\n                dep = x[:, 3:4].to(DEVICE, non_blocking=True)\n                with torch.amp.autocast(\"cuda\", enabled=torch.cuda.is_available()):\n                    logits = model(rgb, dep)\n                    loss = 0.5 * bce(logits, y) + 0.5 * dice(logits, y)\n            else:\n                x = x.to(DEVICE, non_blocking=True)\n                with torch.amp.autocast(\"cuda\", enabled=torch.cuda.is_available()):\n                    logits = model(x)\n                    loss = 0.5 * bce(logits, y) + 0.5 * dice(logits, y)\n\n            loss = loss / max(1, cfg.grad_accum)\n            scaler.scale(loss).backward()\n\n            if (step % cfg.grad_accum) == 0 or step == len(dl_tr):\n                scaler.step(opt)\n                scaler.update()\n                opt.zero_grad(set_to_none=True)\n\n            tr_loss_sum += loss.item() * max(1, cfg.grad_accum)\n            tr_steps += 1\n\n        tr_loss = tr_loss_sum / max(1, tr_steps)\n\n        model.eval()\n        va_loss_sum = 0.0\n        va_steps = 0\n        dice_list = []\n        iou_list = []\n\n        with torch.no_grad():\n            for x, y, sids in dl_va:\n                y = y.to(DEVICE, non_blocking=True)\n\n                if model_key == \"proposed_vit_fusion\":\n                    rgb = x[:, :3].to(DEVICE, non_blocking=True)\n                    dep = x[:, 3:4].to(DEVICE, non_blocking=True)\n                    with torch.amp.autocast(\"cuda\", enabled=torch.cuda.is_available()):\n                        logits = model(rgb, dep)\n                        loss = 0.5 * bce(logits, y) + 0.5 * dice(logits, y)\n                else:\n                    x = x.to(DEVICE, non_blocking=True)\n                    with torch.amp.autocast(\"cuda\", enabled=torch.cuda.is_available()):\n                        logits = model(x)\n                        loss = 0.5 * bce(logits, y) + 0.5 * dice(logits, y)\n\n                va_loss_sum += loss.item()\n                va_steps += 1\n                pred01 = sigmoid_thresh(logits, 0.5)\n                dice_list.append(dice_coeff(pred01, y))\n                iou_list.append(iou_score(pred01, y))\n\n        va_loss = va_loss_sum / max(1, va_steps)\n        va_dice = float(np.mean(dice_list)) if len(dice_list) else np.nan\n        va_iou = float(np.mean(iou_list)) if len(iou_list) else np.nan\n\n        logs.append({\n            \"model\": model_key,\n            \"epoch\": epoch,\n            \"train_loss\": float(tr_loss),\n            \"val_loss\": float(va_loss),\n            \"val_miou\": float(va_iou),\n            \"val_dice\": float(va_dice),\n        })\n\n        ckpt_path = str(ckpt_dir / f\"epoch_{epoch:03d}.pt\")\n        torch.save({\n            \"model_key\": model_key,\n            \"epoch\": epoch,\n            \"state_dict\": model.state_dict(),\n            \"cfg\": cfg.__dict__,\n            \"target_hw\": TARGET_HW,\n            \"seed\": cfg.seed\n        }, ckpt_path)\n\n        if (not np.isnan(va_iou)) and (va_iou > best_metric):\n            best_metric = va_iou\n            best_epoch = epoch\n            best_path = ckpt_path\n\n    train_time_min = (time.time() - start_time) / 60.0\n    peak_vram_gb = (torch.cuda.max_memory_allocated() / (1024**3)) if torch.cuda.is_available() else 0.0\n\n    df_logs = pd.DataFrame(logs)\n    df_logs_path = out_dir / \"epoch_logs.csv\"\n    df_logs.to_csv(df_logs_path, index=False)\n\n    return {\n        \"model\": model_key,\n        \"out_dir\": str(out_dir),\n        \"logs_path\": str(df_logs_path),\n        \"best_ckpt_path\": best_path,\n        \"best_epoch\": int(best_epoch),\n        \"best_metric\": float(best_metric),\n        \"train_time_min\": float(train_time_min),\n        \"peak_vram_gb\": float(peak_vram_gb),\n    }, df_logs\n\ndef predict_on_samples(model_key: str, model: nn.Module, ckpt_path: str, sample_ids: List[str]):\n    model = model.to(DEVICE)\n    ckpt = torch.load(ckpt_path, map_location=DEVICE)\n    model.load_state_dict(ckpt[\"state_dict\"], strict=True)\n    model.eval()\n\n    lut = {df.loc[i, \"sample_id\"]: df.loc[i] for i in range(len(df))}\n    out = []\n\n    for sid in sample_ids:\n        row = lut[sid]\n        rgb = read_rgb(row[\"rgb_path\"])\n        depth = read_depth(row[\"depth_path\"])\n        mask = read_mask(row[\"mask_path\"])\n        rgb, depth, mask = crop_to_min_hw(rgb, depth, mask)\n        rgb, depth, mask = resize_triplet(rgb, depth, mask, TARGET_HW)\n        depth = fill_depth_holes(depth)\n        depth_scaled = robust_depth_scale(depth)\n\n        rgb_t = torch.from_numpy(rgb.astype(np.float32) / 255.0).permute(2,0,1).unsqueeze(0).to(DEVICE)\n        dep_t = torch.from_numpy(depth_scaled.astype(np.float32)).unsqueeze(0).unsqueeze(0).to(DEVICE)\n        gt = mask.astype(np.uint8)\n\n        with torch.no_grad():\n            if model_key == \"proposed_vit_fusion\":\n                logits = model(rgb_t, dep_t)\n            else:\n                x = rgb_t if model_key == \"baseline_a_rgb\" else torch.cat([rgb_t, dep_t], dim=1)\n                logits = model(x)\n            pred = (torch.sigmoid(logits)[0,0].detach().cpu().numpy() >= 0.5).astype(np.uint8)\n\n        out.append((sid, rgb, gt, pred))\n    return out\n\ndef save_qual_grid(sample_ids: List[str], predA, predB, predP, path: Path):\n    mapA = {sid: (rgb, gt, pr) for sid, rgb, gt, pr in predA}\n    mapB = {sid: pr for sid, rgb, gt, pr in predB}\n    mapP = {sid: pr for sid, rgb, gt, pr in predP}\n    cols = 5\n    rows = len(sample_ids)\n    plt.figure(figsize=(3.2 * cols, 2.8 * rows))\n    for r, sid in enumerate(sample_ids):\n        rgb, gt, pA = mapA[sid]\n        pB = mapB[sid]\n        pP = mapP[sid]\n        ax = plt.subplot(rows, cols, r * cols + 1)\n        ax.imshow(rgb)\n        ax.set_title(f\"{sid}\")\n        ax.axis(\"off\")\n        ax = plt.subplot(rows, cols, r * cols + 2)\n        ax.imshow(gt, cmap=\"gray\")\n        ax.set_title(\"GT\")\n        ax.axis(\"off\")\n        ax = plt.subplot(rows, cols, r * cols + 3)\n        ax.imshow(pA, cmap=\"gray\")\n        ax.set_title(\"Baseline A\")\n        ax.axis(\"off\")\n        ax = plt.subplot(rows, cols, r * cols + 4)\n        ax.imshow(pB, cmap=\"gray\")\n        ax.set_title(\"Baseline B\")\n        ax.axis(\"off\")\n        ax = plt.subplot(rows, cols, r * cols + 5)\n        ax.imshow(pP, cmap=\"gray\")\n        ax.set_title(\"Proposed\")\n        ax.axis(\"off\")\n    save_fig(path, dpi=300)\n\ndef draw_architecture_diagram(path: Path):\n    import matplotlib.patches as patches\n    plt.figure(figsize=(12, 4.2))\n    ax = plt.gca()\n    ax.set_xlim(0, 12)\n    ax.set_ylim(0, 4.2)\n    ax.axis(\"off\")\n    def box(x, y, w, h, text):\n        rect = patches.FancyBboxPatch((x,y), w, h, boxstyle=\"round,pad=0.02,rounding_size=0.08\", linewidth=1.2, facecolor=\"white\")\n        ax.add_patch(rect)\n        ax.text(x + w/2, y + h/2, text, ha=\"center\", va=\"center\")\n    def arrow(x1, y1, x2, y2):\n        ax.annotate(\"\", xy=(x2,y2), xytext=(x1,y1), arrowprops=dict(arrowstyle=\"->\", linewidth=1.2))\n    box(0.4, 2.6, 2.0, 0.9, \"RGB\\n(3ch)\")\n    box(0.4, 0.6, 2.0, 0.9, \"Depth\\n(1ch)\")\n    box(2.8, 2.6, 2.2, 0.9, \"ViT encoder\\nRGB\")\n    box(2.8, 0.6, 2.2, 0.9, \"Transformer encoder\\nDepth\")\n    box(5.4, 1.6, 2.1, 1.0, \"Cross-attention\\nfusion\")\n    box(8.0, 1.6, 2.0, 1.0, \"Decoder\\nupsample\")\n    box(10.4, 1.6, 1.2, 1.0, \"Mask\\nlogits\")\n    arrow(2.4, 3.05, 2.8, 3.05)\n    arrow(2.4, 1.05, 2.8, 1.05)\n    arrow(5.0, 3.05, 5.4, 2.1)\n    arrow(5.0, 1.05, 5.4, 2.1)\n    arrow(7.5, 2.1, 8.0, 2.1)\n    arrow(10.0, 2.1, 10.4, 2.1)\n    ax.text(0.4, 4.0, \"Proposed multimodal ViT fusion segmentation model\", ha=\"left\", va=\"center\", fontsize=12)\n    save_fig(path, dpi=300)\n\nBASELINE_EPOCHS = 30\nPROPOSED_EPOCHS = 30\nBATCH_SIZE = 8\nLR = 2e-4\nWD = 1e-4\nOPT = \"adamw\"\nAUG = \"hflip, brightness, rgb_noise\"\nGRAD_ACCUM = 1\n\nbaseline_a_cfg = TrainConfig(epochs=BASELINE_EPOCHS, batch_size=BATCH_SIZE, lr=LR, weight_decay=WD, optimizer=OPT, augmentations=AUG, seed=SEED, grad_accum=GRAD_ACCUM)\nbaseline_b_cfg = TrainConfig(epochs=BASELINE_EPOCHS, batch_size=BATCH_SIZE, lr=LR, weight_decay=WD, optimizer=OPT, augmentations=AUG, seed=SEED, grad_accum=GRAD_ACCUM)\nproposed_cfg   = TrainConfig(epochs=PROPOSED_EPOCHS, batch_size=BATCH_SIZE, lr=LR, weight_decay=WD, optimizer=OPT, augmentations=AUG, seed=SEED, grad_accum=GRAD_ACCUM)\n\nbaseline_a = SimpleUNet(in_ch=3, base=32)\nbaseline_b = SimpleUNet(in_ch=4, base=32)\n\nif not _HAS_TIMM:\n    raise RuntimeError(\"timm is required for proposed model\")\n\nproposed = ProposedViTFusion(\"vit_small_patch16_224\", \"vit_small_patch16_224\", TARGET_HW)\n\ngpu_type = get_gpu_name()\n\nex_rgb = torch.randn(1, 3, TARGET_HW[0], TARGET_HW[1], device=DEVICE)\nex_rgbd = torch.randn(1, 4, TARGET_HW[0], TARGET_HW[1], device=DEVICE)\nex_dep = torch.randn(1, 1, TARGET_HW[0], TARGET_HW[1], device=DEVICE)\n\nsize_rows = []\ndef record_model_size(model_key: str, model_obj: nn.Module, example_inputs: Tuple[torch.Tensor, ...], notes: str):\n    params_m = count_params_m(model_obj)\n    flops_g = try_flops_g(model_obj.to(DEVICE), example_inputs)\n    size_rows.append({\n        \"model\": model_key,\n        \"params_m\": float(params_m),\n        \"flops_g\": float(flops_g) if not np.isnan(flops_g) else np.nan,\n        \"input_resolution\": str(TARGET_HW),\n        \"notes\": notes\n    })\n\nrecord_model_size(\"baseline_a_rgb\", baseline_a, (ex_rgb,), \"SimpleUNet base=32, rgb only\")\nrecord_model_size(\"baseline_b_rgbd\", baseline_b, (ex_rgbd,), \"SimpleUNet base=32, early fusion 4ch\")\nrecord_model_size(\"proposed_vit_fusion\", proposed, (ex_rgb, ex_dep), \"dual ViT + cross-attention + lite decoder\")\n\nmA_info, mA_logs = train_one_model(\"baseline_a_rgb\", baseline_a, use_depth_input=False, cfg=baseline_a_cfg)\nmB_info, mB_logs = train_one_model(\"baseline_b_rgbd\", baseline_b, use_depth_input=True, cfg=baseline_b_cfg)\nmP_info, mP_logs = train_one_model(\"proposed_vit_fusion\", proposed, use_depth_input=True, cfg=proposed_cfg)\n\nbudget_rows = []\nfor info, cfg, tag in [(mA_info, baseline_a_cfg, \"baseline_a_rgb\"),\n                       (mB_info, baseline_b_cfg, \"baseline_b_rgbd\"),\n                       (mP_info, proposed_cfg, \"proposed_vit_fusion\")]:\n    budget_rows.append({\n        \"model\": tag,\n        \"epochs\": int(cfg.epochs),\n        \"batch_size\": int(cfg.batch_size),\n        \"lr\": float(cfg.lr),\n        \"optimizer\": cfg.optimizer,\n        \"augmentations\": cfg.augmentations,\n        \"gpu_type\": gpu_type,\n        \"train_time_min\": float(info[\"train_time_min\"]),\n        \"peak_vram_gb\": float(info[\"peak_vram_gb\"])\n    })\n\ndf_budget = pd.DataFrame(budget_rows)\ndf_size = pd.DataFrame(size_rows)\ndf_best = pd.DataFrame([\n    {\"model\": \"baseline_a_rgb\", \"checkpoint_path\": mA_info[\"best_ckpt_path\"], \"val_best_epoch\": mA_info[\"best_epoch\"], \"val_best_metric\": mA_info[\"best_metric\"], \"seed\": int(SEED)},\n    {\"model\": \"baseline_b_rgbd\", \"checkpoint_path\": mB_info[\"best_ckpt_path\"], \"val_best_epoch\": mB_info[\"best_epoch\"], \"val_best_metric\": mB_info[\"best_metric\"], \"seed\": int(SEED)},\n    {\"model\": \"proposed_vit_fusion\", \"checkpoint_path\": mP_info[\"best_ckpt_path\"], \"val_best_epoch\": mP_info[\"best_epoch\"], \"val_best_metric\": mP_info[\"best_metric\"], \"seed\": int(SEED)},\n])\ndf_logs_all = pd.concat([mA_logs, mB_logs, mP_logs], axis=0).reset_index(drop=True)\n\nTAB_01_BUDGET = PHASE4_DIR / \"tab_01_training_budget_table.csv\"\nTAB_02_SIZE = PHASE4_DIR / \"tab_02_model_size_compute_table.csv\"\nTAB_03_BEST = PHASE4_DIR / \"tab_03_best_checkpoint_index.csv\"\nTAB_04_LOGS = PHASE4_DIR / \"tab_04_batch_level_logs_summary.csv\"\n\ndf_budget.to_csv(TAB_01_BUDGET, index=False)\ndf_size.to_csv(TAB_02_SIZE, index=False)\ndf_best.to_csv(TAB_03_BEST, index=False)\ndf_logs_all.to_csv(TAB_04_LOGS, index=False)\n\ndef plot_loss(df_logs: pd.DataFrame, models: List[str], path: Path, title: str):\n    plt.figure(figsize=(10.5, 4.8))\n    for m in models:\n        d = df_logs[df_logs[\"model\"] == m].sort_values(\"epoch\")\n        plt.plot(d[\"epoch\"], d[\"train_loss\"], label=f\"{m} train\")\n        plt.plot(d[\"epoch\"], d[\"val_loss\"], label=f\"{m} val\")\n    plt.title(title)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    save_fig(path, dpi=300)\n\ndef plot_metrics(df_logs: pd.DataFrame, models: List[str], path: Path, title: str):\n    plt.figure(figsize=(10.5, 4.8))\n    for m in models:\n        d = df_logs[df_logs[\"model\"] == m].sort_values(\"epoch\")\n        plt.plot(d[\"epoch\"], d[\"val_miou\"], label=f\"{m} mIoU\")\n        plt.plot(d[\"epoch\"], d[\"val_dice\"], label=f\"{m} Dice\")\n    plt.title(title)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Metric\")\n    plt.ylim(0, 1.0)\n    plt.legend()\n    save_fig(path, dpi=300)\n\nplot_loss(df_logs_all, [\"baseline_a_rgb\", \"baseline_b_rgbd\"], PHASE4_DIR / \"fig_01_training_curves_baselines.png\", \"Baselines loss curves\")\nplot_metrics(df_logs_all, [\"baseline_a_rgb\", \"baseline_b_rgbd\"], PHASE4_DIR / \"fig_01_training_curves_baselines_metrics.png\", \"Baselines validation metrics\")\nplot_loss(df_logs_all, [\"proposed_vit_fusion\"], PHASE4_DIR / \"fig_02_training_curves_proposed.png\", \"Proposed loss curves\")\nplot_metrics(df_logs_all, [\"proposed_vit_fusion\"], PHASE4_DIR / \"fig_02_training_curves_proposed_metrics.png\", \"Proposed validation metrics\")\n\nif FIG_SAMPLE_IDS_PATH.exists():\n    df_figids = pd.read_csv(FIG_SAMPLE_IDS_PATH)\n    pick = df_figids[df_figids[\"figure_name\"].astype(str).str.contains(\"fig_01_rgb_depth_mask_overlay_grid\", na=False)]\n    if len(pick):\n        ids_str = str(pick.iloc[0][\"sample_id_list\"])\n        qual_ids = [x for x in ids_str.split(\"|\") if x.strip() != \"\"]\n    else:\n        qual_ids = df[df[\"split\"] == \"test\"].sort_values(\"sample_id\", kind=\"mergesort\").head(8)[\"sample_id\"].tolist()\nelse:\n    qual_ids = df[df[\"split\"] == \"test\"].sort_values(\"sample_id\", kind=\"mergesort\").head(8)[\"sample_id\"].tolist()\n\nqual_ids = list(dict.fromkeys(qual_ids))[:8]\nif len(qual_ids) < 4:\n    qual_ids = df[df[\"split\"] == \"test\"].sort_values(\"sample_id\", kind=\"mergesort\").head(8)[\"sample_id\"].tolist()\n\nmA = SimpleUNet(in_ch=3, base=32)\nmB = SimpleUNet(in_ch=4, base=32)\nmP = ProposedViTFusion(\"vit_small_patch16_224\", \"vit_small_patch16_224\", TARGET_HW)\n\nckA = df_best[df_best[\"model\"] == \"baseline_a_rgb\"][\"checkpoint_path\"].iloc[0]\nckB = df_best[df_best[\"model\"] == \"baseline_b_rgbd\"][\"checkpoint_path\"].iloc[0]\nckP = df_best[df_best[\"model\"] == \"proposed_vit_fusion\"][\"checkpoint_path\"].iloc[0]\n\npredA = predict_on_samples(\"baseline_a_rgb\", mA, ckA, qual_ids)\npredB = predict_on_samples(\"baseline_b_rgbd\", mB, ckB, qual_ids)\npredP = predict_on_samples(\"proposed_vit_fusion\", mP, ckP, qual_ids)\n\nsave_qual_grid(qual_ids, predA, predB, predP, PHASE4_DIR / \"fig_03_qualitative_comparison_grid.png\")\ndraw_architecture_diagram(PHASE4_DIR / \"fig_04_model_architecture_diagram.png\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nimport math\nimport random\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom typing import Tuple, List, Dict\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport timm\n\nWORK_ROOT = Path(\"/kaggle/working\")\nPHASE1_DIR = WORK_ROOT / \"results\" / \"01_data_integrity_and_profile\"\nPHASE2_DIR = WORK_ROOT / \"results\" / \"02_sensor_alignment_and_quality\"\nPHASE3_DIR = WORK_ROOT / \"results\" / \"03_preprocess_and_split_protocol\"\nPHASE4_DIR = WORK_ROOT / \"results\" / \"04_training_baselines_and_vit_fusion\"\nPHASE4_DIR.mkdir(parents=True, exist_ok=True)\n\nMANIFEST_VALID_PATH = PHASE1_DIR / \"tab_01_dataset_manifest_valid_only.csv\"\nSPLIT_MANIFEST_PATH = PHASE3_DIR / \"tab_03_split_manifest.csv\"\nPREPROCESS_CONFIG_PATH = PHASE3_DIR / \"tab_01_preprocess_config_table.csv\"\nFIG_SAMPLE_IDS_PATH = PHASE2_DIR / \"tab_04_figure_sample_ids.csv\"\n\ndf_valid = pd.read_csv(MANIFEST_VALID_PATH).sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\ndf_split = pd.read_csv(SPLIT_MANIFEST_PATH).sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\ndf = df_valid.merge(df_split, on=[\"sample_id\", \"site_id\"], how=\"inner\").sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\n\nTARGET_HW = (422, 640)\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEED = 1337\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(SEED)\n\nplt.rcParams.update({\n    \"font.size\": 10,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 10,\n    \"xtick.labelsize\": 9,\n    \"ytick.labelsize\": 9,\n    \"legend.fontsize\": 9,\n})\n\ndef save_fig(path: Path, dpi=300):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    plt.tight_layout()\n    plt.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n    plt.close()\n\ndef read_rgb(path: str):\n    img = Image.open(path)\n    img.load()\n    arr = np.array(img)\n    if arr.ndim == 2:\n        arr = np.stack([arr, arr, arr], axis=2)\n    if arr.shape[2] > 3:\n        arr = arr[:, :, :3]\n    return arr.astype(np.uint8)\n\ndef read_depth(path: str):\n    img = Image.open(path)\n    img.load()\n    arr = np.array(img)\n    if arr.ndim == 3:\n        arr = arr[:, :, 0]\n    return arr\n\ndef read_mask(path: str):\n    img = Image.open(path)\n    img.load()\n    arr = np.array(img)\n    if arr.ndim == 3:\n        arr = np.any(arr > 0, axis=2).astype(np.uint8)\n    else:\n        arr = (arr > 0).astype(np.uint8)\n    return arr\n\ndef crop_to_min_hw(rgb, depth, mask):\n    h = min(rgb.shape[0], depth.shape[0], mask.shape[0])\n    w = min(rgb.shape[1], depth.shape[1], mask.shape[1])\n    return rgb[:h, :w], depth[:h, :w], mask[:h, :w]\n\ndef resize_triplet(rgb, depth, mask01, out_hw):\n    oh, ow = int(out_hw[0]), int(out_hw[1])\n    rgb_r = cv2.resize(rgb, (ow, oh), interpolation=cv2.INTER_LINEAR)\n    depth_f = depth.astype(np.float32)\n    depth_r = cv2.resize(depth_f, (ow, oh), interpolation=cv2.INTER_NEAREST)\n    mask_r = cv2.resize(mask01.astype(np.uint8), (ow, oh), interpolation=cv2.INTER_NEAREST)\n    mask_r = (mask_r > 0).astype(np.uint8)\n    return rgb_r, depth_r, mask_r\n\ndef fill_depth_holes(depth: np.ndarray):\n    d = depth.astype(np.float32)\n    valid = (d > 0) & np.isfinite(d)\n    if np.count_nonzero(valid) < 10:\n        return d\n    invalid = (~valid).astype(np.uint8)\n    d0 = d.copy()\n    d0[~valid] = 0.0\n    filled = cv2.inpaint(d0, invalid, 3, cv2.INPAINT_TELEA)\n    filled[~np.isfinite(filled)] = 0.0\n    return filled\n\ndef robust_depth_scale(depth: np.ndarray):\n    d = depth.astype(np.float32)\n    valid = (d > 0) & np.isfinite(d)\n    if np.count_nonzero(valid) < 10:\n        return np.zeros_like(d, dtype=np.float32)\n    vv = d[valid]\n    mn, mx = float(np.percentile(vv, 1)), float(np.percentile(vv, 99))\n    if mx - mn < 1e-6:\n        out = np.zeros_like(d, dtype=np.float32)\n        out[valid] = 0.5\n        return out\n    out = (d - mn) / (mx - mn)\n    out = np.clip(out, 0.0, 1.0)\n    out[~valid] = 0.0\n    return out.astype(np.float32)\n\ndef augment_pair(rgb_u8, depth_f, mask_u8, rng: np.random.Generator):\n    if rng.random() < 0.5:\n        rgb_u8 = np.ascontiguousarray(rgb_u8[:, ::-1])\n        depth_f = np.ascontiguousarray(depth_f[:, ::-1])\n        mask_u8 = np.ascontiguousarray(mask_u8[:, ::-1])\n    if rng.random() < 0.25:\n        factor = 0.8 + 0.4 * rng.random()\n        rgb_f = rgb_u8.astype(np.float32) * factor\n        rgb_u8 = np.clip(rgb_f, 0, 255).astype(np.uint8)\n    if rng.random() < 0.15:\n        noise = rng.normal(0, 3.0, size=rgb_u8.shape).astype(np.float32)\n        rgb_u8 = np.clip(rgb_u8.astype(np.float32) + noise, 0, 255).astype(np.uint8)\n    return rgb_u8, depth_f, mask_u8\n\nclass ISODSegDataset(Dataset):\n    def __init__(self, df_in: pd.DataFrame, split: str, target_hw: Tuple[int,int], seed: int, use_depth: bool, vit_hw: Tuple[int,int]):\n        self.df = df_in[df_in[\"split\"] == split].sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\n        self.target_hw = target_hw\n        self.use_depth = use_depth\n        self.rng = np.random.default_rng(seed + (0 if split == \"train\" else 999))\n        self.split = split\n        self.vit_hw = vit_hw\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.loc[idx]\n        rgb = read_rgb(row[\"rgb_path\"])\n        depth = read_depth(row[\"depth_path\"])\n        mask = read_mask(row[\"mask_path\"])\n        rgb, depth, mask = crop_to_min_hw(rgb, depth, mask)\n        rgb, depth, mask = resize_triplet(rgb, depth, mask, self.target_hw)\n        depth = fill_depth_holes(depth)\n        depth_scaled = robust_depth_scale(depth)\n        if self.split == \"train\":\n            rgb, depth_scaled, mask = augment_pair(rgb, depth_scaled, mask, self.rng)\n        rgb_t = torch.from_numpy(rgb.astype(np.float32) / 255.0).permute(2,0,1)\n        mask_t = torch.from_numpy(mask.astype(np.float32)).unsqueeze(0)\n        dep_t = torch.from_numpy(depth_scaled.astype(np.float32)).unsqueeze(0)\n        x = torch.cat([rgb_t, dep_t], dim=0) if self.use_depth else rgb_t\n        return x, mask_t, str(row[\"sample_id\"])\n\ndef sigmoid_thresh(x, thr=0.5):\n    return (torch.sigmoid(x) >= thr).float()\n\ndef dice_coeff(pred01, gt01, eps=1e-6):\n    inter = (pred01 * gt01).sum(dim=(1,2,3))\n    denom = pred01.sum(dim=(1,2,3)) + gt01.sum(dim=(1,2,3))\n    return ((2.0 * inter + eps) / (denom + eps)).mean().item()\n\ndef iou_score(pred01, gt01, eps=1e-6):\n    inter = (pred01 * gt01).sum(dim=(1,2,3))\n    union = ((pred01 + gt01) > 0).float().sum(dim=(1,2,3))\n    return ((inter + eps) / (union + eps)).mean().item()\n\nclass DiceLoss(nn.Module):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n    def forward(self, logits, targets):\n        probs = torch.sigmoid(logits)\n        num = 2.0 * (probs * targets).sum(dim=(1,2,3)) + self.eps\n        den = probs.sum(dim=(1,2,3)) + targets.sum(dim=(1,2,3)) + self.eps\n        return (1.0 - (num / den)).mean()\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        )\n    def forward(self, x):\n        return self.net(x)\n\ndef center_crop_or_pad_to(x, ref):\n    _, _, h, w = x.shape\n    _, _, hr, wr = ref.shape\n    if h == hr and w == wr:\n        return x\n    if h > hr or w > wr:\n        dh = max(0, h - hr)\n        dw = max(0, w - wr)\n        top = dh // 2\n        left = dw // 2\n        x = x[:, :, top:top+hr, left:left+wr]\n    _, _, h2, w2 = x.shape\n    if h2 < hr or w2 < wr:\n        ph = max(0, hr - h2)\n        pw = max(0, wr - w2)\n        pad = (pw//2, pw - pw//2, ph//2, ph - ph//2)\n        x = F.pad(x, pad, mode=\"replicate\")\n    return x\n\nclass SimpleUNet(nn.Module):\n    def __init__(self, in_ch=3, base=32):\n        super().__init__()\n        self.enc1 = ConvBlock(in_ch, base)\n        self.pool1 = nn.MaxPool2d(2, ceil_mode=True)\n        self.enc2 = ConvBlock(base, base*2)\n        self.pool2 = nn.MaxPool2d(2, ceil_mode=True)\n        self.enc3 = ConvBlock(base*2, base*4)\n        self.pool3 = nn.MaxPool2d(2, ceil_mode=True)\n        self.bott = ConvBlock(base*4, base*8)\n        self.up3 = nn.ConvTranspose2d(base*8, base*4, 2, stride=2)\n        self.dec3 = ConvBlock(base*8, base*4)\n        self.up2 = nn.ConvTranspose2d(base*4, base*2, 2, stride=2)\n        self.dec2 = ConvBlock(base*4, base*2)\n        self.up1 = nn.ConvTranspose2d(base*2, base, 2, stride=2)\n        self.dec1 = ConvBlock(base*2, base)\n        self.head = nn.Conv2d(base, 1, 1)\n    def forward(self, x):\n        e1 = self.enc1(x)\n        e2 = self.enc2(self.pool1(e1))\n        e3 = self.enc3(self.pool2(e2))\n        b = self.bott(self.pool3(e3))\n        d3 = self.up3(b)\n        e3a = center_crop_or_pad_to(e3, d3)\n        d3 = self.dec3(torch.cat([d3, e3a], dim=1))\n        d2 = self.up2(d3)\n        e2a = center_crop_or_pad_to(e2, d2)\n        d2 = self.dec2(torch.cat([d2, e2a], dim=1))\n        d1 = self.up1(d2)\n        e1a = center_crop_or_pad_to(e1, d1)\n        d1 = self.dec1(torch.cat([d1, e1a], dim=1))\n        out = self.head(d1)\n        out = F.interpolate(out, size=(x.shape[2], x.shape[3]), mode=\"bilinear\", align_corners=False)\n        return out\n\nclass ViTEncoderFeature(nn.Module):\n    def __init__(self, model_name: str, in_chans: int, img_hw: Tuple[int,int]):\n        super().__init__()\n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=True,\n            in_chans=in_chans,\n            img_size=img_hw,\n            dynamic_img_size=True,\n            dynamic_img_pad=True,\n            features_only=True,\n            out_indices=(3,)\n        )\n        self.out_ch = self.backbone.feature_info.channels()[-1]\n    def forward(self, x):\n        feats = self.backbone(x)\n        return feats[-1]\n\nclass CrossAttentionFusion(nn.Module):\n    def __init__(self, ch: int, heads: int = 8):\n        super().__init__()\n        self.q = nn.Conv2d(ch, ch, 1, bias=False)\n        self.k = nn.Conv2d(ch, ch, 1, bias=False)\n        self.v = nn.Conv2d(ch, ch, 1, bias=False)\n        self.attn = nn.MultiheadAttention(embed_dim=ch, num_heads=heads, batch_first=True)\n        self.proj = nn.Conv2d(ch, ch, 1, bias=False)\n        self.norm = nn.LayerNorm(ch)\n    def forward(self, frgb, fdep):\n        b, c, h, w = frgb.shape\n        q = self.q(frgb).flatten(2).transpose(1,2)\n        k = self.k(fdep).flatten(2).transpose(1,2)\n        v = self.v(fdep).flatten(2).transpose(1,2)\n        qn = self.norm(q)\n        out, _ = self.attn(qn, k, v, need_weights=False)\n        out = out.transpose(1,2).reshape(b, c, h, w)\n        out = self.proj(out)\n        return frgb + out\n\nclass LiteDecoder(nn.Module):\n    def __init__(self, in_ch: int, out_hw: Tuple[int,int]):\n        super().__init__()\n        self.out_hw = out_hw\n        self.conv1 = ConvBlock(in_ch, 256)\n        self.up1 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        self.conv2 = ConvBlock(128, 128)\n        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n        self.conv3 = ConvBlock(64, 64)\n        self.up3 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n        self.conv4 = ConvBlock(32, 32)\n        self.head = nn.Conv2d(32, 1, 1)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.up1(x)\n        x = self.conv2(x)\n        x = self.up2(x)\n        x = self.conv3(x)\n        x = self.up3(x)\n        x = self.conv4(x)\n        x = self.head(x)\n        x = F.interpolate(x, size=self.out_hw, mode=\"bilinear\", align_corners=False)\n        return x\n\nclass ProposedViTFusion(nn.Module):\n    def __init__(self, rgb_name: str, dep_name: str, full_hw: Tuple[int,int], vit_hw: Tuple[int,int]):\n        super().__init__()\n        self.full_hw = full_hw\n        self.vit_hw = vit_hw\n        self.rgb_enc = ViTEncoderFeature(rgb_name, in_chans=3, img_hw=vit_hw)\n        self.dep_enc = ViTEncoderFeature(dep_name, in_chans=1, img_hw=vit_hw)\n        ch = self.rgb_enc.out_ch\n        self.dep_proj = nn.Conv2d(self.dep_enc.out_ch, ch, 1, bias=False) if self.dep_enc.out_ch != ch else nn.Identity()\n        self.fuse = CrossAttentionFusion(ch, heads=8)\n        self.dec = LiteDecoder(ch, out_hw=full_hw)\n    def forward(self, rgb3, dep1):\n        rgb_small = F.interpolate(rgb3, size=self.vit_hw, mode=\"bilinear\", align_corners=False)\n        dep_small = F.interpolate(dep1, size=self.vit_hw, mode=\"bilinear\", align_corners=False)\n        fr = self.rgb_enc(rgb_small)\n        fd = self.dep_proj(self.dep_enc(dep_small))\n        f = self.fuse(fr, fd)\n        return self.dec(f)\n\n@dataclass\nclass TrainConfig:\n    epochs: int\n    batch_size: int\n    lr: float\n    weight_decay: float\n    optimizer: str\n    augmentations: str\n    seed: int\n    grad_accum: int\n\ndef make_loader(split: str, batch_size: int, use_depth: bool, vit_hw: Tuple[int,int]):\n    ds = ISODSegDataset(df, split=split, target_hw=TARGET_HW, seed=SEED, use_depth=use_depth, vit_hw=vit_hw)\n    shuffle = (split == \"train\")\n    dl = DataLoader(ds, batch_size=batch_size, shuffle=shuffle, num_workers=2, pin_memory=True, drop_last=False)\n    return ds, dl\n\ndef dice_loss(logits, targets, eps=1e-6):\n    probs = torch.sigmoid(logits)\n    num = 2.0 * (probs * targets).sum(dim=(1,2,3)) + eps\n    den = probs.sum(dim=(1,2,3)) + targets.sum(dim=(1,2,3)) + eps\n    return (1.0 - (num / den)).mean()\n\ndef train_one_model(model_key: str, model: nn.Module, use_depth_input: bool, cfg: TrainConfig, vit_hw: Tuple[int,int], resume_ckpt: str = \"\"):\n    out_dir = PHASE4_DIR / model_key\n    out_dir.mkdir(parents=True, exist_ok=True)\n    ckpt_dir = out_dir / \"checkpoints\"\n    ckpt_dir.mkdir(parents=True, exist_ok=True)\n\n    model = model.to(DEVICE)\n\n    _, dl_tr = make_loader(\"train\", cfg.batch_size, use_depth=use_depth_input, vit_hw=vit_hw)\n    _, dl_va = make_loader(\"val\", cfg.batch_size, use_depth=use_depth_input, vit_hw=vit_hw)\n\n    if cfg.optimizer.lower() == \"adamw\":\n        opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n    else:\n        opt = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n\n    start_epoch = 1\n    best_metric = -1.0\n    best_epoch = -1\n    best_path = \"\"\n\n    if resume_ckpt and Path(resume_ckpt).exists():\n        ck = torch.load(resume_ckpt, map_location=DEVICE)\n        model.load_state_dict(ck[\"state_dict\"], strict=True)\n        if \"opt_state\" in ck and ck[\"opt_state\"] is not None:\n            opt.load_state_dict(ck[\"opt_state\"])\n        start_epoch = int(ck.get(\"epoch\", 0)) + 1\n        best_metric = float(ck.get(\"best_metric\", -1.0))\n        best_epoch = int(ck.get(\"best_epoch\", -1))\n        best_path = str(ck.get(\"best_path\", \"\"))\n\n    logs_path = out_dir / \"epoch_logs.csv\"\n    if logs_path.exists():\n        df_logs = pd.read_csv(logs_path)\n        logs = df_logs.to_dict(orient=\"records\")\n    else:\n        logs = []\n\n    scaler = torch.amp.GradScaler(\"cuda\", enabled=torch.cuda.is_available())\n\n    start_time = time.time()\n    if torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats()\n\n    for epoch in range(start_epoch, cfg.epochs + 1):\n        model.train()\n        tr_loss_sum = 0.0\n        tr_steps = 0\n        opt.zero_grad(set_to_none=True)\n\n        for step, (x, y, sids) in enumerate(dl_tr, start=1):\n            y = y.to(DEVICE, non_blocking=True)\n\n            if model_key == \"proposed_vit_fusion\":\n                rgb = x[:, :3].to(DEVICE, non_blocking=True)\n                dep = x[:, 3:4].to(DEVICE, non_blocking=True)\n                with torch.amp.autocast(\"cuda\", enabled=torch.cuda.is_available()):\n                    logits = model(rgb, dep)\n                    loss = 0.5 * F.binary_cross_entropy_with_logits(logits, y) + 0.5 * dice_loss(logits, y)\n            else:\n                x = x.to(DEVICE, non_blocking=True)\n                with torch.amp.autocast(\"cuda\", enabled=torch.cuda.is_available()):\n                    logits = model(x)\n                    loss = 0.5 * F.binary_cross_entropy_with_logits(logits, y) + 0.5 * dice_loss(logits, y)\n\n            loss = loss / max(1, cfg.grad_accum)\n            scaler.scale(loss).backward()\n\n            if (step % cfg.grad_accum) == 0 or step == len(dl_tr):\n                scaler.step(opt)\n                scaler.update()\n                opt.zero_grad(set_to_none=True)\n\n            tr_loss_sum += loss.item() * max(1, cfg.grad_accum)\n            tr_steps += 1\n\n        tr_loss = tr_loss_sum / max(1, tr_steps)\n\n        model.eval()\n        va_loss_sum = 0.0\n        va_steps = 0\n        dice_list = []\n        iou_list = []\n\n        with torch.no_grad():\n            for x, y, sids in dl_va:\n                y = y.to(DEVICE, non_blocking=True)\n                if model_key == \"proposed_vit_fusion\":\n                    rgb = x[:, :3].to(DEVICE, non_blocking=True)\n                    dep = x[:, 3:4].to(DEVICE, non_blocking=True)\n                    with torch.amp.autocast(\"cuda\", enabled=torch.cuda.is_available()):\n                        logits = model(rgb, dep)\n                        loss = 0.5 * F.binary_cross_entropy_with_logits(logits, y) + 0.5 * dice_loss(logits, y)\n                else:\n                    x = x.to(DEVICE, non_blocking=True)\n                    with torch.amp.autocast(\"cuda\", enabled=torch.cuda.is_available()):\n                        logits = model(x)\n                        loss = 0.5 * F.binary_cross_entropy_with_logits(logits, y) + 0.5 * dice_loss(logits, y)\n\n                va_loss_sum += loss.item()\n                va_steps += 1\n                pred01 = (torch.sigmoid(logits) >= 0.5).float()\n                dice_list.append(dice_coeff(pred01, y))\n                iou_list.append(iou_score(pred01, y))\n\n        va_loss = va_loss_sum / max(1, va_steps)\n        va_dice = float(np.mean(dice_list)) if len(dice_list) else np.nan\n        va_iou = float(np.mean(iou_list)) if len(iou_list) else np.nan\n\n        logs.append({\n            \"model\": model_key,\n            \"epoch\": int(epoch),\n            \"train_loss\": float(tr_loss),\n            \"val_loss\": float(va_loss),\n            \"val_miou\": float(va_iou),\n            \"val_dice\": float(va_dice),\n        })\n        pd.DataFrame(logs).to_csv(logs_path, index=False)\n\n        ckpt_path = str(ckpt_dir / f\"epoch_{epoch:03d}.pt\")\n        torch.save({\n            \"model_key\": model_key,\n            \"epoch\": int(epoch),\n            \"state_dict\": model.state_dict(),\n            \"opt_state\": opt.state_dict(),\n            \"cfg\": cfg.__dict__,\n            \"target_hw\": TARGET_HW,\n            \"vit_hw\": vit_hw,\n            \"seed\": cfg.seed,\n            \"best_metric\": float(best_metric),\n            \"best_epoch\": int(best_epoch),\n            \"best_path\": str(best_path),\n        }, ckpt_path)\n\n        if (not np.isnan(va_iou)) and (va_iou > best_metric):\n            best_metric = va_iou\n            best_epoch = epoch\n            best_path = ckpt_path\n\n    train_time_min = (time.time() - start_time) / 60.0\n    peak_vram_gb = (torch.cuda.max_memory_allocated() / (1024**3)) if torch.cuda.is_available() else 0.0\n\n    return {\n        \"model\": model_key,\n        \"out_dir\": str(out_dir),\n        \"logs_path\": str(logs_path),\n        \"best_ckpt_path\": str(best_path),\n        \"best_epoch\": int(best_epoch),\n        \"best_metric\": float(best_metric),\n        \"train_time_min\": float(train_time_min),\n        \"peak_vram_gb\": float(peak_vram_gb),\n        \"vit_hw\": vit_hw,\n    }\n\ndef latest_checkpoint(ckpt_dir: Path):\n    if not ckpt_dir.exists():\n        return \"\"\n    pts = sorted(ckpt_dir.glob(\"epoch_*.pt\"))\n    if not pts:\n        return \"\"\n    def ep(p):\n        m = re.search(r\"epoch_(\\d+)\\.pt$\", p.name)\n        return int(m.group(1)) if m else -1\n    pts = sorted(pts, key=lambda p: ep(p))\n    return str(pts[-1])\n\ndef plot_loss(df_logs: pd.DataFrame, models: List[str], path: Path, title: str):\n    plt.figure(figsize=(10.5, 4.8))\n    for m in models:\n        d = df_logs[df_logs[\"model\"] == m].sort_values(\"epoch\")\n        plt.plot(d[\"epoch\"], d[\"train_loss\"], label=f\"{m} train\")\n        plt.plot(d[\"epoch\"], d[\"val_loss\"], label=f\"{m} val\")\n    plt.title(title)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    save_fig(path, dpi=300)\n\ndef plot_metrics(df_logs: pd.DataFrame, models: List[str], path: Path, title: str):\n    plt.figure(figsize=(10.5, 4.8))\n    for m in models:\n        d = df_logs[df_logs[\"model\"] == m].sort_values(\"epoch\")\n        plt.plot(d[\"epoch\"], d[\"val_miou\"], label=f\"{m} mIoU\")\n        plt.plot(d[\"epoch\"], d[\"val_dice\"], label=f\"{m} Dice\")\n    plt.title(title)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Metric\")\n    plt.ylim(0, 1.0)\n    plt.legend()\n    save_fig(path, dpi=300)\n\nbaseline_a_cfg = TrainConfig(epochs=30, batch_size=8, lr=2e-4, weight_decay=1e-4, optimizer=\"adamw\", augmentations=\"hflip, brightness, rgb_noise\", seed=SEED, grad_accum=1)\nbaseline_b_cfg = TrainConfig(epochs=30, batch_size=8, lr=2e-4, weight_decay=1e-4, optimizer=\"adamw\", augmentations=\"hflip, brightness, rgb_noise\", seed=SEED, grad_accum=1)\nproposed_cfg = TrainConfig(epochs=30, batch_size=8, lr=2e-4, weight_decay=1e-4, optimizer=\"adamw\", augmentations=\"hflip, brightness, rgb_noise\", seed=SEED, grad_accum=1)\n\nbaseline_a = SimpleUNet(in_ch=3, base=32)\nbaseline_b = SimpleUNet(in_ch=4, base=32)\n\nvit_hw = (224, 224)\nproposed = ProposedViTFusion(\"vit_small_patch16_224\", \"vit_small_patch16_224\", full_hw=TARGET_HW, vit_hw=vit_hw)\n\nbaseline_a_dir = PHASE4_DIR / \"baseline_a_rgb\" / \"checkpoints\"\nbaseline_b_dir = PHASE4_DIR / \"baseline_b_rgbd\" / \"checkpoints\"\nproposed_dir = PHASE4_DIR / \"proposed_vit_fusion\" / \"checkpoints\"\n\nresume_a = latest_checkpoint(baseline_a_dir)\nresume_b = latest_checkpoint(baseline_b_dir)\nresume_p = latest_checkpoint(proposed_dir)\n\nmA = train_one_model(\"baseline_a_rgb\", baseline_a, use_depth_input=False, cfg=baseline_a_cfg, vit_hw=vit_hw, resume_ckpt=resume_a)\nmB = train_one_model(\"baseline_b_rgbd\", baseline_b, use_depth_input=True, cfg=baseline_b_cfg, vit_hw=vit_hw, resume_ckpt=resume_b)\nmP = train_one_model(\"proposed_vit_fusion\", proposed, use_depth_input=True, cfg=proposed_cfg, vit_hw=vit_hw, resume_ckpt=resume_p)\n\nlogs_all = []\nfor k in [\"baseline_a_rgb\", \"baseline_b_rgbd\", \"proposed_vit_fusion\"]:\n    p = PHASE4_DIR / k / \"epoch_logs.csv\"\n    if p.exists():\n        logs_all.append(pd.read_csv(p))\ndf_logs_all = pd.concat(logs_all, axis=0).reset_index(drop=True) if len(logs_all) else pd.DataFrame(columns=[\"model\",\"epoch\",\"train_loss\",\"val_loss\",\"val_miou\",\"val_dice\"])\n\nTAB_01_BUDGET = PHASE4_DIR / \"tab_01_training_budget_table.csv\"\nTAB_02_SIZE = PHASE4_DIR / \"tab_02_model_size_compute_table.csv\"\nTAB_03_BEST = PHASE4_DIR / \"tab_03_best_checkpoint_index.csv\"\nTAB_04_LOGS = PHASE4_DIR / \"tab_04_batch_level_logs_summary.csv\"\n\ngpu_type = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\"\n\ndef best_from_dir(model_key: str):\n    p = PHASE4_DIR / model_key / \"epoch_logs.csv\"\n    if not p.exists():\n        return {\"model\": model_key, \"checkpoint_path\": \"\", \"val_best_epoch\": -1, \"val_best_metric\": -1.0, \"seed\": int(SEED)}\n    d = pd.read_csv(p)\n    d = d.dropna(subset=[\"val_miou\"])\n    if len(d) == 0:\n        return {\"model\": model_key, \"checkpoint_path\": \"\", \"val_best_epoch\": -1, \"val_best_metric\": -1.0, \"seed\": int(SEED)}\n    best_row = d.sort_values([\"val_miou\",\"epoch\"], ascending=[False, True]).iloc[0]\n    ep = int(best_row[\"epoch\"])\n    ck = PHASE4_DIR / model_key / \"checkpoints\" / f\"epoch_{ep:03d}.pt\"\n    return {\"model\": model_key, \"checkpoint_path\": str(ck), \"val_best_epoch\": ep, \"val_best_metric\": float(best_row[\"val_miou\"]), \"seed\": int(SEED)}\n\ndf_best = pd.DataFrame([\n    best_from_dir(\"baseline_a_rgb\"),\n    best_from_dir(\"baseline_b_rgbd\"),\n    best_from_dir(\"proposed_vit_fusion\"),\n])\ndf_best.to_csv(TAB_03_BEST, index=False)\n\ndf_logs_all.to_csv(TAB_04_LOGS, index=False)\n\nplot_loss(df_logs_all, [\"baseline_a_rgb\",\"baseline_b_rgbd\"], PHASE4_DIR / \"fig_01_training_curves_baselines.png\", \"Baselines loss curves\")\nplot_metrics(df_logs_all, [\"baseline_a_rgb\",\"baseline_b_rgbd\"], PHASE4_DIR / \"fig_01_training_curves_baselines_metrics.png\", \"Baselines validation metrics\")\nplot_loss(df_logs_all, [\"proposed_vit_fusion\"], PHASE4_DIR / \"fig_02_training_curves_proposed.png\", \"Proposed loss curves\")\nplot_metrics(df_logs_all, [\"proposed_vit_fusion\"], PHASE4_DIR / \"fig_02_training_curves_proposed_metrics.png\", \"Proposed validation metrics\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nimport math\nimport random\nimport json\nimport re\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom typing import Tuple, List, Dict\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport timm\n\nWORK_ROOT = Path(\"/kaggle/working\")\nPHASE1_DIR = WORK_ROOT / \"results\" / \"01_data_integrity_and_profile\"\nPHASE2_DIR = WORK_ROOT / \"results\" / \"02_sensor_alignment_and_quality\"\nPHASE3_DIR = WORK_ROOT / \"results\" / \"03_preprocess_and_split_protocol\"\nPHASE4_DIR = WORK_ROOT / \"results\" / \"04_training_baselines_and_vit_fusion\"\nPHASE5_DIR = WORK_ROOT / \"results\" / \"05_eval_ablation_robust_explain\"\nPHASE5_DIR.mkdir(parents=True, exist_ok=True)\n\nMANIFEST_VALID_PATH = PHASE1_DIR / \"tab_01_dataset_manifest_valid_only.csv\"\nSPLIT_MANIFEST_PATH = PHASE3_DIR / \"tab_03_split_manifest.csv\"\nPHASE4_BEST_PATH = PHASE4_DIR / \"tab_03_best_checkpoint_index.csv\"\nPHASE4_SIZE_PATH = PHASE4_DIR / \"tab_02_model_size_compute_table.csv\"\nFIG_SAMPLE_IDS_PATH = PHASE2_DIR / \"tab_04_figure_sample_ids.csv\"\n\nTARGET_HW = (422, 640)\nVIT_HW = (224, 224)\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEED = 1337\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nplt.rcParams.update({\n    \"font.size\": 10,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 10,\n    \"xtick.labelsize\": 9,\n    \"ytick.labelsize\": 9,\n    \"legend.fontsize\": 9,\n})\n\ndef save_fig(path: Path, dpi=300):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    plt.tight_layout()\n    plt.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n    plt.close()\n\ndef read_rgb(path: str):\n    img = Image.open(path)\n    img.load()\n    arr = np.array(img)\n    if arr.ndim == 2:\n        arr = np.stack([arr, arr, arr], axis=2)\n    if arr.shape[2] > 3:\n        arr = arr[:, :, :3]\n    return arr.astype(np.uint8)\n\ndef read_depth(path: str):\n    img = Image.open(path)\n    img.load()\n    arr = np.array(img)\n    if arr.ndim == 3:\n        arr = arr[:, :, 0]\n    return arr\n\ndef read_mask(path: str):\n    img = Image.open(path)\n    img.load()\n    arr = np.array(img)\n    if arr.ndim == 3:\n        arr = np.any(arr > 0, axis=2).astype(np.uint8)\n    else:\n        arr = (arr > 0).astype(np.uint8)\n    return arr\n\ndef crop_to_min_hw(rgb, depth, mask):\n    h = min(rgb.shape[0], depth.shape[0], mask.shape[0])\n    w = min(rgb.shape[1], depth.shape[1], mask.shape[1])\n    return rgb[:h, :w], depth[:h, :w], mask[:h, :w]\n\ndef resize_triplet(rgb, depth, mask01, out_hw):\n    oh, ow = int(out_hw[0]), int(out_hw[1])\n    rgb_r = cv2.resize(rgb, (ow, oh), interpolation=cv2.INTER_LINEAR)\n    depth_f = depth.astype(np.float32)\n    depth_r = cv2.resize(depth_f, (ow, oh), interpolation=cv2.INTER_NEAREST)\n    mask_r = cv2.resize(mask01.astype(np.uint8), (ow, oh), interpolation=cv2.INTER_NEAREST)\n    mask_r = (mask_r > 0).astype(np.uint8)\n    return rgb_r, depth_r, mask_r\n\ndef fill_depth_holes(depth: np.ndarray):\n    d = depth.astype(np.float32)\n    valid = (d > 0) & np.isfinite(d)\n    if np.count_nonzero(valid) < 10:\n        return d\n    invalid = (~valid).astype(np.uint8)\n    d0 = d.copy()\n    d0[~valid] = 0.0\n    filled = cv2.inpaint(d0, invalid, 3, cv2.INPAINT_TELEA)\n    filled[~np.isfinite(filled)] = 0.0\n    return filled\n\ndef robust_depth_scale(depth: np.ndarray):\n    d = depth.astype(np.float32)\n    valid = (d > 0) & np.isfinite(d)\n    if np.count_nonzero(valid) < 10:\n        return np.zeros_like(d, dtype=np.float32)\n    vv = d[valid]\n    mn, mx = float(np.percentile(vv, 1)), float(np.percentile(vv, 99))\n    if mx - mn < 1e-6:\n        out = np.zeros_like(d, dtype=np.float32)\n        out[valid] = 0.5\n        return out\n    out = (d - mn) / (mx - mn)\n    out = np.clip(out, 0.0, 1.0)\n    out[~valid] = 0.0\n    return out.astype(np.float32)\n\ndef metric_one(pred01, gt01, eps=1e-6):\n    pred = pred01.astype(np.uint8)\n    gt = gt01.astype(np.uint8)\n    tp = np.logical_and(pred == 1, gt == 1).sum()\n    fp = np.logical_and(pred == 1, gt == 0).sum()\n    fn = np.logical_and(pred == 0, gt == 1).sum()\n    inter = tp\n    union = np.logical_or(pred == 1, gt == 1).sum()\n    miou = (inter + eps) / (union + eps)\n    dice = (2 * inter + eps) / (pred.sum() + gt.sum() + eps)\n    prec = (tp + eps) / (tp + fp + eps)\n    rec = (tp + eps) / (tp + fn + eps)\n    f1 = (2 * prec * rec + eps) / (prec + rec + eps)\n    return float(miou), float(dice), float(prec), float(rec), float(f1)\n\ndef apply_corruption(rgb_u8, dep_scaled, condition: str, severity: int, rng: np.random.Generator):\n    rgb = rgb_u8.copy()\n    dep = dep_scaled.copy()\n    if condition == \"clean\":\n        return rgb, dep\n    if condition == \"depth_missing\":\n        return rgb, np.zeros_like(dep, dtype=np.float32)\n    if condition == \"depth_noise\":\n        sigma = [0.02, 0.05, 0.10][severity-1]\n        dep = np.clip(dep + rng.normal(0, sigma, size=dep.shape).astype(np.float32), 0.0, 1.0)\n        return rgb, dep\n    if condition == \"depth_holes\":\n        rate = [0.05, 0.15, 0.30][severity-1]\n        hole = rng.random(dep.shape) < rate\n        dep = dep.copy()\n        dep[hole] = 0.0\n        return rgb, dep\n    if condition == \"rgb_lowlight\":\n        gamma = [1.6, 2.0, 2.4][severity-1]\n        rgb_f = (rgb.astype(np.float32) / 255.0) ** gamma\n        rgb = np.clip(rgb_f * 255.0, 0, 255).astype(np.uint8)\n        return rgb, dep\n    if condition == \"rgb_blur\":\n        k = [3, 5, 7][severity-1]\n        rgb = cv2.GaussianBlur(rgb, (k, k), 0)\n        return rgb, dep\n    return rgb, dep\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        )\n    def forward(self, x):\n        return self.net(x)\n\ndef center_crop_or_pad_to(x, ref):\n    _, _, h, w = x.shape\n    _, _, hr, wr = ref.shape\n    if h == hr and w == wr:\n        return x\n    if h > hr or w > wr:\n        dh = max(0, h - hr)\n        dw = max(0, w - wr)\n        top = dh // 2\n        left = dw // 2\n        x = x[:, :, top:top+hr, left:left+wr]\n    _, _, h2, w2 = x.shape\n    if h2 < hr or w2 < wr:\n        ph = max(0, hr - h2)\n        pw = max(0, wr - w2)\n        pad = (pw//2, pw - pw//2, ph//2, ph - ph//2)\n        x = F.pad(x, pad, mode=\"replicate\")\n    return x\n\nclass SimpleUNet(nn.Module):\n    def __init__(self, in_ch=3, base=32):\n        super().__init__()\n        self.enc1 = ConvBlock(in_ch, base)\n        self.pool1 = nn.MaxPool2d(2, ceil_mode=True)\n        self.enc2 = ConvBlock(base, base*2)\n        self.pool2 = nn.MaxPool2d(2, ceil_mode=True)\n        self.enc3 = ConvBlock(base*2, base*4)\n        self.pool3 = nn.MaxPool2d(2, ceil_mode=True)\n        self.bott = ConvBlock(base*4, base*8)\n        self.up3 = nn.ConvTranspose2d(base*8, base*4, 2, stride=2)\n        self.dec3 = ConvBlock(base*8, base*4)\n        self.up2 = nn.ConvTranspose2d(base*4, base*2, 2, stride=2)\n        self.dec2 = ConvBlock(base*4, base*2)\n        self.up1 = nn.ConvTranspose2d(base*2, base, 2, stride=2)\n        self.dec1 = ConvBlock(base*2, base)\n        self.head = nn.Conv2d(base, 1, 1)\n    def forward(self, x):\n        e1 = self.enc1(x)\n        e2 = self.enc2(self.pool1(e1))\n        e3 = self.enc3(self.pool2(e2))\n        b = self.bott(self.pool3(e3))\n        d3 = self.up3(b)\n        e3a = center_crop_or_pad_to(e3, d3)\n        d3 = self.dec3(torch.cat([d3, e3a], dim=1))\n        d2 = self.up2(d3)\n        e2a = center_crop_or_pad_to(e2, d2)\n        d2 = self.dec2(torch.cat([d2, e2a], dim=1))\n        d1 = self.up1(d2)\n        e1a = center_crop_or_pad_to(e1, d1)\n        d1 = self.dec1(torch.cat([d1, e1a], dim=1))\n        out = self.head(d1)\n        out = F.interpolate(out, size=(x.shape[2], x.shape[3]), mode=\"bilinear\", align_corners=False)\n        return out\n\nclass ViTEncoderFeature(nn.Module):\n    def __init__(self, model_name: str, in_chans: int, img_hw: Tuple[int,int]):\n        super().__init__()\n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=True,\n            in_chans=in_chans,\n            img_size=img_hw,\n            dynamic_img_size=True,\n            dynamic_img_pad=True,\n            features_only=True,\n            out_indices=(3,)\n        )\n        self.out_ch = self.backbone.feature_info.channels()[-1]\n    def forward(self, x):\n        feats = self.backbone(x)\n        return feats[-1]\n\nclass CrossAttentionFusion(nn.Module):\n    def __init__(self, ch: int, heads: int = 8):\n        super().__init__()\n        self.q = nn.Conv2d(ch, ch, 1, bias=False)\n        self.k = nn.Conv2d(ch, ch, 1, bias=False)\n        self.v = nn.Conv2d(ch, ch, 1, bias=False)\n        self.attn = nn.MultiheadAttention(embed_dim=ch, num_heads=heads, batch_first=True)\n        self.proj = nn.Conv2d(ch, ch, 1, bias=False)\n        self.norm = nn.LayerNorm(ch)\n    def forward(self, frgb, fdep):\n        b, c, h, w = frgb.shape\n        q = self.q(frgb).flatten(2).transpose(1,2)\n        k = self.k(fdep).flatten(2).transpose(1,2)\n        v = self.v(fdep).flatten(2).transpose(1,2)\n        qn = self.norm(q)\n        out, _ = self.attn(qn, k, v, need_weights=False)\n        out = out.transpose(1,2).reshape(b, c, h, w)\n        out = self.proj(out)\n        return frgb + out\n\nclass LiteDecoder(nn.Module):\n    def __init__(self, in_ch: int, out_hw: Tuple[int,int]):\n        super().__init__()\n        self.out_hw = out_hw\n        self.conv1 = ConvBlock(in_ch, 256)\n        self.up1 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        self.conv2 = ConvBlock(128, 128)\n        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n        self.conv3 = ConvBlock(64, 64)\n        self.up3 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n        self.conv4 = ConvBlock(32, 32)\n        self.head = nn.Conv2d(32, 1, 1)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.up1(x)\n        x = self.conv2(x)\n        x = self.up2(x)\n        x = self.conv3(x)\n        x = self.up3(x)\n        x = self.conv4(x)\n        x = self.head(x)\n        x = F.interpolate(x, size=self.out_hw, mode=\"bilinear\", align_corners=False)\n        return x\n\nclass ProposedViTFusion(nn.Module):\n    def __init__(self, rgb_name: str, dep_name: str, full_hw: Tuple[int,int], vit_hw: Tuple[int,int], depth_tokens_stride: int = 1, mode: str = \"fusion\"):\n        super().__init__()\n        self.full_hw = full_hw\n        self.vit_hw = vit_hw\n        self.mode = mode\n        self.depth_tokens_stride = depth_tokens_stride\n        self.rgb_enc = ViTEncoderFeature(rgb_name, in_chans=3, img_hw=vit_hw)\n        self.dep_enc = ViTEncoderFeature(dep_name, in_chans=1, img_hw=vit_hw)\n        ch = self.rgb_enc.out_ch\n        self.dep_proj = nn.Conv2d(self.dep_enc.out_ch, ch, 1, bias=False) if self.dep_enc.out_ch != ch else nn.Identity()\n        self.fuse = CrossAttentionFusion(ch, heads=8)\n        self.dec = LiteDecoder(ch, out_hw=full_hw)\n        self.late_gate = nn.Sequential(nn.Conv2d(ch * 2, ch, 1, bias=False), nn.ReLU(inplace=True))\n    def forward_feats(self, rgb3, dep1):\n        rgb_small = F.interpolate(rgb3, size=self.vit_hw, mode=\"bilinear\", align_corners=False)\n        dep_small = F.interpolate(dep1, size=self.vit_hw, mode=\"bilinear\", align_corners=False)\n        fr = self.rgb_enc(rgb_small)\n        fd = self.dep_proj(self.dep_enc(dep_small))\n        if self.depth_tokens_stride > 1:\n            fd = fd[:, :, ::self.depth_tokens_stride, ::self.depth_tokens_stride]\n            fd = F.interpolate(fd, size=fr.shape[-2:], mode=\"bilinear\", align_corners=False)\n        return fr, fd\n    def forward(self, rgb3, dep1):\n        fr, fd = self.forward_feats(rgb3, dep1)\n        if self.mode == \"rgb_only\":\n            f = fr\n        elif self.mode == \"depth_only\":\n            f = fd\n        elif self.mode == \"no_fusion\":\n            f = fr\n        elif self.mode == \"late_fusion\":\n            f = self.late_gate(torch.cat([fr, fd], dim=1))\n        else:\n            f = self.fuse(fr, fd)\n        return self.dec(f)\n\ndef load_ckpt(model: nn.Module, ckpt_path: str):\n    ck = torch.load(ckpt_path, map_location=DEVICE)\n    missing, unexpected = model.load_state_dict(ck[\"state_dict\"], strict=False)\n    model.to(DEVICE)\n    model.eval()\n    return {\"missing_keys\": list(missing), \"unexpected_keys\": list(unexpected)}\n\ndef get_best_ckpt_paths():\n    df_best = pd.read_csv(PHASE4_BEST_PATH)\n    m = {r[\"model\"]: r[\"checkpoint_path\"] for _, r in df_best.iterrows()}\n    return m\n\ndef get_params_lookup():\n    if PHASE4_SIZE_PATH.exists():\n        d = pd.read_csv(PHASE4_SIZE_PATH)\n        return {r[\"model\"]: float(r[\"params_m\"]) for _, r in d.iterrows()}\n    return {}\n\ndf_valid = pd.read_csv(MANIFEST_VALID_PATH).sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\ndf_split = pd.read_csv(SPLIT_MANIFEST_PATH).sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\ndf_all = df_valid.merge(df_split, on=[\"sample_id\",\"site_id\"], how=\"inner\").sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\ndf_test = df_all[df_all[\"split\"] == \"test\"].sort_values(\"sample_id\", kind=\"mergesort\").reset_index(drop=True)\n\nbest_paths = get_best_ckpt_paths()\nparams_lookup = get_params_lookup()\n\nck_a = best_paths.get(\"baseline_a_rgb\",\"\")\nck_b = best_paths.get(\"baseline_b_rgbd\",\"\")\nck_p = best_paths.get(\"proposed_vit_fusion\",\"\")\n\nif not ck_a or not Path(ck_a).exists():\n    raise FileNotFoundError(\"Missing baseline_a_rgb checkpoint in tab_03_best_checkpoint_index.csv\")\nif not ck_b or not Path(ck_b).exists():\n    raise FileNotFoundError(\"Missing baseline_b_rgbd checkpoint in tab_03_best_checkpoint_index.csv\")\nif not ck_p or not Path(ck_p).exists():\n    raise FileNotFoundError(\"Missing proposed_vit_fusion checkpoint in tab_03_best_checkpoint_index.csv\")\n\nbaseline_a = SimpleUNet(in_ch=3, base=32)\nbaseline_b = SimpleUNet(in_ch=4, base=32)\nproposed_fusion = ProposedViTFusion(\"vit_small_patch16_224\", \"vit_small_patch16_224\", full_hw=TARGET_HW, vit_hw=VIT_HW, depth_tokens_stride=1, mode=\"fusion\")\n\ninfo_a = load_ckpt(baseline_a, ck_a)\ninfo_b = load_ckpt(baseline_b, ck_b)\ninfo_p = load_ckpt(proposed_fusion, ck_p)\n\nrng_global = np.random.default_rng(SEED)\n\ndef infer_mask(model_key: str, model: nn.Module, rgb_u8, dep_scaled):\n    rgb_t = torch.from_numpy(rgb_u8.astype(np.float32)/255.0).permute(2,0,1).unsqueeze(0).to(DEVICE)\n    dep_t = torch.from_numpy(dep_scaled.astype(np.float32)).unsqueeze(0).unsqueeze(0).to(DEVICE)\n    with torch.no_grad():\n        if model_key == \"baseline_a_rgb\":\n            logits = model(rgb_t)\n        elif model_key == \"baseline_b_rgbd\":\n            logits = model(torch.cat([rgb_t, dep_t], dim=1))\n        else:\n            logits = model(rgb_t, dep_t)\n    pred01 = (torch.sigmoid(logits)[0,0].detach().cpu().numpy() >= 0.5).astype(np.uint8)\n    return pred01\n\ndef eval_models_on_test(models: Dict[str, nn.Module], condition: str = \"clean\", severity: int = 1):\n    rows = []\n    for i in range(len(df_test)):\n        r = df_test.loc[i]\n        sid = str(r[\"sample_id\"])\n        site = str(r[\"site_id\"])\n        rgb = read_rgb(r[\"rgb_path\"])\n        dep = read_depth(r[\"depth_path\"])\n        msk = read_mask(r[\"mask_path\"])\n        rgb, dep, msk = crop_to_min_hw(rgb, dep, msk)\n        rgb, dep, msk = resize_triplet(rgb, dep, msk, TARGET_HW)\n        dep = fill_depth_holes(dep)\n        dep_scaled = robust_depth_scale(dep)\n        rgb_c, dep_c = apply_corruption(rgb, dep_scaled, condition, severity, rng_global)\n        for mk, mo in models.items():\n            pred = infer_mask(mk, mo, rgb_c, dep_c)\n            miou, dice, prec, rec, f1 = metric_one(pred, msk)\n            rows.append({\n                \"sample_id\": sid,\n                \"site_id\": site,\n                \"model\": mk,\n                \"miou\": miou,\n                \"dice\": dice,\n                \"precision\": prec,\n                \"recall\": rec,\n                \"f1\": f1,\n                \"notes\": f\"{condition}|{severity}\"\n            })\n    return pd.DataFrame(rows).sort_values([\"model\",\"sample_id\"], kind=\"mergesort\").reset_index(drop=True)\n\nmodels_main = {\n    \"baseline_a_rgb\": baseline_a,\n    \"baseline_b_rgbd\": baseline_b,\n    \"proposed_vit_fusion\": proposed_fusion\n}\n\nTAB_01_TEST_RESULTS = PHASE5_DIR / \"tab_01_test_results_full.csv\"\nTAB_02_MAIN = PHASE5_DIR / \"tab_02_main_results_table.csv\"\nTAB_03_ABL = PHASE5_DIR / \"tab_03_ablation_table.csv\"\nTAB_04_ROB = PHASE5_DIR / \"tab_04_robustness_table.csv\"\n\nFIG_01_MAIN = PHASE5_DIR / \"fig_01_main_results_bar.png\"\nFIG_02_ABL = PHASE5_DIR / \"fig_02_ablation_impact_plot.png\"\nFIG_03_ROB = PHASE5_DIR / \"fig_03_robustness_curves.png\"\nFIG_04_EXPLAIN = PHASE5_DIR / \"fig_04_explainability_examples.png\"\n\ndf_clean_full = eval_models_on_test(models_main, condition=\"clean\", severity=1)\ndf_clean_full.to_csv(TAB_01_TEST_RESULTS, index=False)\n\ndef summarize_main(df_full: pd.DataFrame, params_lookup: Dict[str,float]):\n    out = []\n    for mk in sorted(df_full[\"model\"].unique()):\n        d = df_full[df_full[\"model\"] == mk]\n        out.append({\n            \"method\": mk,\n            \"modalities\": \"rgb\" if mk == \"baseline_a_rgb\" else \"rgb+depth\",\n            \"miou\": float(d[\"miou\"].mean()),\n            \"dice\": float(d[\"dice\"].mean()),\n            \"precision\": float(d[\"precision\"].mean()),\n            \"recall\": float(d[\"recall\"].mean()),\n            \"f1\": float(d[\"f1\"].mean()),\n            \"params_m\": float(params_lookup.get(mk, np.nan))\n        })\n    return pd.DataFrame(out)\n\ndf_main = summarize_main(df_clean_full, params_lookup)\ndf_main.to_csv(TAB_02_MAIN, index=False)\n\ndef fig_main_bar(df_main: pd.DataFrame, path: Path):\n    plt.figure(figsize=(8.2, 4.2))\n    x = np.arange(len(df_main))\n    plt.bar(x - 0.15, df_main[\"miou\"].values, width=0.3, label=\"mIoU\")\n    plt.bar(x + 0.15, df_main[\"dice\"].values, width=0.3, label=\"Dice\")\n    plt.xticks(x, df_main[\"method\"].values, rotation=20, ha=\"right\")\n    plt.ylim(0, 1.0)\n    plt.ylabel(\"Score\")\n    plt.title(\"Test performance\")\n    plt.legend()\n    save_fig(path, dpi=300)\n\nfig_main_bar(df_main, FIG_01_MAIN)\n\ndef build_ablation_models():\n    ablations = {}\n    ablations[\"proposed_fusion\"] = ProposedViTFusion(\"vit_small_patch16_224\",\"vit_small_patch16_224\", TARGET_HW, VIT_HW, depth_tokens_stride=1, mode=\"fusion\")\n    ablations[\"no_fusion\"] = ProposedViTFusion(\"vit_small_patch16_224\",\"vit_small_patch16_224\", TARGET_HW, VIT_HW, depth_tokens_stride=1, mode=\"no_fusion\")\n    ablations[\"late_fusion\"] = ProposedViTFusion(\"vit_small_patch16_224\",\"vit_small_patch16_224\", TARGET_HW, VIT_HW, depth_tokens_stride=1, mode=\"late_fusion\")\n    ablations[\"rgb_only_vit\"] = ProposedViTFusion(\"vit_small_patch16_224\",\"vit_small_patch16_224\", TARGET_HW, VIT_HW, depth_tokens_stride=1, mode=\"rgb_only\")\n    ablations[\"depth_only\"] = ProposedViTFusion(\"vit_small_patch16_224\",\"vit_small_patch16_224\", TARGET_HW, VIT_HW, depth_tokens_stride=1, mode=\"depth_only\")\n    ablations[\"reduced_depth_tokens_x2\"] = ProposedViTFusion(\"vit_small_patch16_224\",\"vit_small_patch16_224\", TARGET_HW, VIT_HW, depth_tokens_stride=2, mode=\"fusion\")\n    return ablations\n\nabl_models = build_ablation_models()\nabl_load_infos = {}\nfor k in list(abl_models.keys()):\n    abl_load_infos[k] = load_ckpt(abl_models[k], ck_p)\n\ndef eval_ablation(abl_models: Dict[str, nn.Module]):\n    rows = []\n    for name, model in abl_models.items():\n        df_full = eval_models_on_test({name: model}, condition=\"clean\", severity=1)\n        miou = float(df_full[\"miou\"].mean())\n        dice = float(df_full[\"dice\"].mean())\n        rows.append({\"ablation_name\": name, \"miou\": miou, \"dice\": dice})\n    df_ab = pd.DataFrame(rows).sort_values(\"ablation_name\", kind=\"mergesort\").reset_index(drop=True)\n    base = float(df_ab[df_ab[\"ablation_name\"] == \"proposed_fusion\"][\"miou\"].iloc[0]) if (df_ab[\"ablation_name\"] == \"proposed_fusion\").any() else float(df_ab[\"miou\"].max())\n    df_ab[\"delta_miou\"] = df_ab[\"miou\"].astype(float) - base\n    df_ab[\"notes\"] = \"\"\n    return df_ab\n\ndf_ablation = eval_ablation(abl_models)\ndf_ablation.to_csv(TAB_03_ABL, index=False)\n\ndef fig_ablation(df_ab: pd.DataFrame, path: Path):\n    plt.figure(figsize=(9.0, 4.2))\n    x = np.arange(len(df_ab))\n    plt.bar(x, df_ab[\"miou\"].values)\n    plt.xticks(x, df_ab[\"ablation_name\"].values, rotation=25, ha=\"right\")\n    plt.ylim(0, 1.0)\n    plt.ylabel(\"mIoU\")\n    plt.title(\"Ablation impact on mIoU (test)\")\n    save_fig(path, dpi=300)\n\nfig_ablation(df_ablation, FIG_02_ABL)\n\nrobust_conditions = [\n    (\"depth_missing\", [1]),\n    (\"depth_noise\", [1,2,3]),\n    (\"depth_holes\", [1,2,3]),\n    (\"rgb_lowlight\", [1,2,3]),\n    (\"rgb_blur\", [1,2,3]),\n]\n\ndef robustness_table(models: Dict[str, nn.Module]):\n    rows = []\n    base_df = eval_models_on_test(models, condition=\"clean\", severity=1)\n    base = {mk: float(base_df[base_df[\"model\"] == mk][\"miou\"].mean()) for mk in models.keys()}\n    for cond, sevs in robust_conditions:\n        for sev in sevs:\n            df_full = eval_models_on_test(models, condition=cond, severity=sev)\n            for mk in models.keys():\n                d = df_full[df_full[\"model\"] == mk]\n                miou = float(d[\"miou\"].mean())\n                dice = float(d[\"dice\"].mean())\n                rows.append({\n                    \"condition\": cond,\n                    \"severity\": int(sev),\n                    \"model\": mk,\n                    \"miou\": miou,\n                    \"dice\": dice,\n                    \"delta_from_clean\": float(miou - base[mk]),\n                    \"notes\": \"\"\n                })\n    return pd.DataFrame(rows).sort_values([\"model\",\"condition\",\"severity\"], kind=\"mergesort\").reset_index(drop=True)\n\ndf_rob = robustness_table(models_main)\ndf_rob.to_csv(TAB_04_ROB, index=False)\n\ndef fig_robustness(df_rob: pd.DataFrame, path: Path, model_key=\"proposed_vit_fusion\"):\n    plt.figure(figsize=(9.2, 4.8))\n    d = df_rob[df_rob[\"model\"] == model_key].copy()\n    labels = []\n    ys = []\n    for cond, sevs in robust_conditions:\n        for sev in sevs:\n            r = d[(d[\"condition\"] == cond) & (d[\"severity\"] == sev)]\n            if len(r):\n                labels.append(f\"{cond}:{sev}\")\n                ys.append(float(r[\"miou\"].iloc[0]))\n    x = np.arange(len(labels))\n    plt.plot(x, ys, marker=\"o\", label=f\"{model_key} mIoU\")\n    plt.xticks(x, labels, rotation=30, ha=\"right\")\n    plt.ylim(0, 1.0)\n    plt.ylabel(\"mIoU\")\n    plt.title(\"Robustness under corruptions\")\n    plt.legend()\n    save_fig(path, dpi=300)\n\nfig_robustness(df_rob, FIG_03_ROB, model_key=\"proposed_vit_fusion\")\n\ndef explain_map_from_fused_feats(model: ProposedViTFusion, rgb_u8, dep_scaled):\n    rgb_t = torch.from_numpy(rgb_u8.astype(np.float32)/255.0).permute(2,0,1).unsqueeze(0).to(DEVICE)\n    dep_t = torch.from_numpy(dep_scaled.astype(np.float32)).unsqueeze(0).unsqueeze(0).to(DEVICE)\n    with torch.no_grad():\n        fr, fd = model.forward_feats(rgb_t, dep_t)\n        f = model.fuse(fr, fd)\n        mag = torch.mean(torch.abs(f), dim=1)[0].detach().cpu().numpy()\n    mag = (mag - mag.min()) / (mag.max() - mag.min() + 1e-6)\n    mag = cv2.resize(mag, (TARGET_HW[1], TARGET_HW[0]), interpolation=cv2.INTER_LINEAR)\n    return mag\n\ndef pick_explain_ids(k=8):\n    if FIG_SAMPLE_IDS_PATH.exists():\n        df_fig = pd.read_csv(FIG_SAMPLE_IDS_PATH)\n        pick = df_fig[df_fig[\"figure_name\"].astype(str).str.contains(\"outliers\", na=False)]\n        if len(pick):\n            ids_str = str(pick.iloc[0][\"sample_id_list\"])\n            ids = [x for x in ids_str.split(\"|\") if x.strip() != \"\"]\n            if len(ids) >= k:\n                return ids[:k]\n    return df_test.sort_values(\"sample_id\", kind=\"mergesort\").head(k)[\"sample_id\"].tolist()\n\nEXPLAIN_IDS = pick_explain_ids(k=8)\n\ndef fig_explainability(sample_ids: List[str], path: Path):\n    lut = {df_test.loc[i, \"sample_id\"]: df_test.loc[i] for i in range(len(df_test))}\n    rows = len(sample_ids)\n    cols = 4\n    plt.figure(figsize=(3.2 * cols, 2.8 * rows))\n    for r, sid in enumerate(sample_ids):\n        row = lut[sid]\n        rgb = read_rgb(row[\"rgb_path\"])\n        dep = read_depth(row[\"depth_path\"])\n        msk = read_mask(row[\"mask_path\"])\n        rgb, dep, msk = crop_to_min_hw(rgb, dep, msk)\n        rgb, dep, msk = resize_triplet(rgb, dep, msk, TARGET_HW)\n        dep = fill_depth_holes(dep)\n        dep_scaled = robust_depth_scale(dep)\n        pred = infer_mask(\"proposed_vit_fusion\", proposed_fusion, rgb, dep_scaled)\n        att = explain_map_from_fused_feats(proposed_fusion, rgb, dep_scaled)\n        overlay = rgb.copy().astype(np.float32)\n        heat = (att * 255.0).astype(np.uint8)\n        heat = cv2.applyColorMap(heat, cv2.COLORMAP_JET)\n        overlay = np.clip(0.65 * overlay + 0.35 * heat.astype(np.float32), 0, 255).astype(np.uint8)\n\n        ax = plt.subplot(rows, cols, r*cols + 1)\n        ax.imshow(rgb)\n        ax.set_title(f\"{sid}\")\n        ax.axis(\"off\")\n\n        ax = plt.subplot(rows, cols, r*cols + 2)\n        ax.imshow(msk, cmap=\"gray\")\n        ax.set_title(\"GT\")\n        ax.axis(\"off\")\n\n        ax = plt.subplot(rows, cols, r*cols + 3)\n        ax.imshow(pred, cmap=\"gray\")\n        ax.set_title(\"Pred\")\n        ax.axis(\"off\")\n\n        ax = plt.subplot(rows, cols, r*cols + 4)\n        ax.imshow(overlay)\n        ax.set_title(\"Explain\")\n        ax.axis(\"off\")\n    save_fig(path, dpi=300)\n\nfig_explainability(EXPLAIN_IDS, FIG_04_EXPLAIN)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EXPLAIN_IDS = [str(x) for x in EXPLAIN_IDS]\ndf_test[\"sample_id\"] = df_test[\"sample_id\"].astype(str)\n\ntest_ids = set(df_test[\"sample_id\"].tolist())\nexplain_ids_ok = [sid for sid in EXPLAIN_IDS if sid in test_ids]\n\nif len(explain_ids_ok) == 0:\n    explain_ids_ok = df_test.sort_values(\"sample_id\", kind=\"mergesort\").head(8)[\"sample_id\"].astype(str).tolist()\n\nTAB_05_EXPLAIN_IDS = PHASE5_DIR / \"tab_05_explainability_sample_ids.csv\"\npd.DataFrame({\n    \"figure_name\": [\"fig_04_explainability_examples.png\"],\n    \"sample_id_list\": [\"|\".join(explain_ids_ok)],\n    \"selection_rule\": [\"phase2_ids_intersect_test_else_first8_test_sorted\"]\n}).to_csv(TAB_05_EXPLAIN_IDS, index=False)\n\nEXPLAIN_IDS = explain_ids_ok\n\nfig_explainability(EXPLAIN_IDS, FIG_04_EXPLAIN)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nfrom pathlib import Path\nfrom typing import Tuple\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport psutil\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport timm\n\nWORK_ROOT = Path(\"/kaggle/working\")\nRESULTS_ROOT = WORK_ROOT / \"results\"\nPHASE4_DIR = RESULTS_ROOT / \"04_training_baselines_and_vit_fusion\"\nPHASE5_DIR = RESULTS_ROOT / \"05_eval_ablation_robust_explain\"\nPHASE6_DIR = RESULTS_ROOT / \"06_edge_and_final_ieee_package\"\nPHASE6_DIR.mkdir(parents=True, exist_ok=True)\n\nTARGET_HW = (422, 640)\nVIT_HW = (224, 224)\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nSEED = 1337\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nplt.rcParams.update({\n    \"font.size\": 10,\n    \"axes.titlesize\": 11,\n    \"axes.labelsize\": 10,\n    \"xtick.labelsize\": 9,\n    \"ytick.labelsize\": 9,\n    \"legend.fontsize\": 9,\n})\n\ndef save_fig(path: Path, dpi=300):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    plt.tight_layout()\n    plt.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n    plt.close()\n\ndef file_size_mb(p: Path):\n    if not p.exists():\n        return float(\"nan\")\n    return float(p.stat().st_size) / (1024**2)\n\ndef params_m(model: nn.Module):\n    return float(sum(p.numel() for p in model.parameters())) / 1e6\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass ViTEncoderFeature(nn.Module):\n    def __init__(self, model_name: str, in_chans: int, img_hw: Tuple[int,int]):\n        super().__init__()\n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=True,\n            in_chans=in_chans,\n            img_size=img_hw,\n            dynamic_img_size=True,\n            dynamic_img_pad=True,\n            features_only=True,\n            out_indices=(3,)\n        )\n        self.out_ch = self.backbone.feature_info.channels()[-1]\n    def forward(self, x):\n        feats = self.backbone(x)\n        return feats[-1]\n\nclass CrossAttentionFusion(nn.Module):\n    def __init__(self, ch: int, heads: int = 8):\n        super().__init__()\n        self.q = nn.Conv2d(ch, ch, 1, bias=False)\n        self.k = nn.Conv2d(ch, ch, 1, bias=False)\n        self.v = nn.Conv2d(ch, ch, 1, bias=False)\n        self.attn = nn.MultiheadAttention(embed_dim=ch, num_heads=heads, batch_first=True)\n        self.proj = nn.Conv2d(ch, ch, 1, bias=False)\n        self.norm = nn.LayerNorm(ch)\n    def forward(self, frgb, fdep):\n        b, c, h, w = frgb.shape\n        q = self.q(frgb).flatten(2).transpose(1,2)\n        k = self.k(fdep).flatten(2).transpose(1,2)\n        v = self.v(fdep).flatten(2).transpose(1,2)\n        qn = self.norm(q)\n        out, _ = self.attn(qn, k, v, need_weights=False)\n        out = out.transpose(1,2).reshape(b, c, h, w)\n        out = self.proj(out)\n        return frgb + out\n\nclass LiteDecoder(nn.Module):\n    def __init__(self, in_ch: int, out_hw: Tuple[int,int]):\n        super().__init__()\n        self.out_hw = out_hw\n        self.conv1 = ConvBlock(in_ch, 256)\n        self.up1 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n        self.conv2 = ConvBlock(128, 128)\n        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n        self.conv3 = ConvBlock(64, 64)\n        self.up3 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n        self.conv4 = ConvBlock(32, 32)\n        self.head = nn.Conv2d(32, 1, 1)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.up1(x)\n        x = self.conv2(x)\n        x = self.up2(x)\n        x = self.conv3(x)\n        x = self.up3(x)\n        x = self.conv4(x)\n        x = self.head(x)\n        x = F.interpolate(x, size=self.out_hw, mode=\"bilinear\", align_corners=False)\n        return x\n\nclass ProposedViTFusion(nn.Module):\n    def __init__(self, rgb_name: str, dep_name: str, full_hw: Tuple[int,int], vit_hw: Tuple[int,int], depth_tokens_stride: int = 1, mode: str = \"fusion\"):\n        super().__init__()\n        self.full_hw = full_hw\n        self.vit_hw = vit_hw\n        self.mode = mode\n        self.depth_tokens_stride = depth_tokens_stride\n        self.rgb_enc = ViTEncoderFeature(rgb_name, in_chans=3, img_hw=vit_hw)\n        self.dep_enc = ViTEncoderFeature(dep_name, in_chans=1, img_hw=vit_hw)\n        ch = self.rgb_enc.out_ch\n        self.dep_proj = nn.Conv2d(self.dep_enc.out_ch, ch, 1, bias=False) if self.dep_enc.out_ch != ch else nn.Identity()\n        self.fuse = CrossAttentionFusion(ch, heads=8)\n        self.dec = LiteDecoder(ch, out_hw=full_hw)\n        self.late_gate = nn.Sequential(nn.Conv2d(ch * 2, ch, 1, bias=False), nn.ReLU(inplace=True))\n    def forward_feats(self, rgb3, dep1):\n        rgb_small = F.interpolate(rgb3, size=self.vit_hw, mode=\"bilinear\", align_corners=False)\n        dep_small = F.interpolate(dep1, size=self.vit_hw, mode=\"bilinear\", align_corners=False)\n        fr = self.rgb_enc(rgb_small)\n        fd = self.dep_proj(self.dep_enc(dep_small))\n        if self.depth_tokens_stride > 1:\n            fd = fd[:, :, ::self.depth_tokens_stride, ::self.depth_tokens_stride]\n            fd = F.interpolate(fd, size=fr.shape[-2:], mode=\"bilinear\", align_corners=False)\n        return fr, fd\n    def forward(self, rgb3, dep1):\n        fr, fd = self.forward_feats(rgb3, dep1)\n        if self.mode == \"rgb_only\":\n            f = fr\n        elif self.mode == \"depth_only\":\n            f = fd\n        elif self.mode == \"no_fusion\":\n            f = fr\n        elif self.mode == \"late_fusion\":\n            f = self.late_gate(torch.cat([fr, fd], dim=1))\n        else:\n            f = self.fuse(fr, fd)\n        return self.dec(f)\n\ndef load_best_ckpt_path():\n    p = PHASE4_DIR / \"tab_03_best_checkpoint_index.csv\"\n    if not p.exists():\n        raise FileNotFoundError(str(p))\n    d = pd.read_csv(p)\n    row = d[d[\"model\"] == \"proposed_vit_fusion\"]\n    if len(row) == 0:\n        raise RuntimeError(\"proposed_vit_fusion not found in tab_03_best_checkpoint_index.csv\")\n    ck = str(row.iloc[0][\"checkpoint_path\"])\n    if not ck or not Path(ck).exists():\n        raise FileNotFoundError(ck)\n    return ck\n\ndef load_ckpt_non_strict(model: nn.Module, ckpt_path: str):\n    ck = torch.load(ckpt_path, map_location=DEVICE)\n    model.load_state_dict(ck[\"state_dict\"], strict=False)\n    model.to(DEVICE)\n    model.eval()\n    return ck\n\nck_p = load_best_ckpt_path()\nproposed = ProposedViTFusion(\"vit_small_patch16_224\", \"vit_small_patch16_224\", full_hw=TARGET_HW, vit_hw=VIT_HW, depth_tokens_stride=1, mode=\"fusion\")\n_ = load_ckpt_non_strict(proposed, ck_p)\n\nART_DIR = PHASE6_DIR / \"artifacts\"\nART_DIR.mkdir(parents=True, exist_ok=True)\n\nONNX_FP32 = ART_DIR / \"proposed_vit_fusion_fp32.onnx\"\nONNX_FP16 = ART_DIR / \"proposed_vit_fusion_fp16.onnx\"\nONNX_INT8 = ART_DIR / \"proposed_vit_fusion_int8_dynamic.onnx\"\n\nx_rgb_fp32 = torch.randn(1, 3, TARGET_HW[0], TARGET_HW[1], device=DEVICE, dtype=torch.float32)\nx_dep_fp32 = torch.randn(1, 1, TARGET_HW[0], TARGET_HW[1], device=DEVICE, dtype=torch.float32)\nx_rgb_fp16 = x_rgb_fp32.half()\nx_dep_fp16 = x_dep_fp32.half()\n\ndef export_onnx_fp32(model: nn.Module, path: Path, opset: int = 17):\n    m = model.to(DEVICE).eval().float()\n    torch.onnx.export(\n        m,\n        (x_rgb_fp32, x_dep_fp32),\n        str(path),\n        export_params=True,\n        opset_version=opset,\n        do_constant_folding=True,\n        input_names=[\"rgb\", \"depth\"],\n        output_names=[\"logits\"],\n        dynamic_axes=None,\n    )\n\ndef export_onnx_fp16(model: nn.Module, path: Path, opset: int = 17):\n    m = model.to(DEVICE).eval().half()\n    torch.onnx.export(\n        m,\n        (x_rgb_fp16, x_dep_fp16),\n        str(path),\n        export_params=True,\n        opset_version=opset,\n        do_constant_folding=True,\n        input_names=[\"rgb\", \"depth\"],\n        output_names=[\"logits\"],\n        dynamic_axes=None,\n    )\n\nexport_ok_fp32 = True\nexport_ok_fp16 = True\n\ntry:\n    export_onnx_fp32(proposed, ONNX_FP32, opset=17)\nexcept Exception as e:\n    export_ok_fp32 = False\n    with open(ART_DIR / \"export_fp32_error.txt\", \"w\") as f:\n        f.write(str(e))\n\ntry:\n    export_onnx_fp16(proposed, ONNX_FP16, opset=17)\nexcept Exception as e:\n    export_ok_fp16 = False\n    with open(ART_DIR / \"export_fp16_error.txt\", \"w\") as f:\n        f.write(str(e))\n\nquant_ok = False\nquant_notes = \"\"\ntry:\n    import onnx\n    from onnxruntime.quantization import quantize_dynamic, QuantType\n    if export_ok_fp32 and ONNX_FP32.exists():\n        quantize_dynamic(str(ONNX_FP32), str(ONNX_INT8), weight_type=QuantType.QInt8)\n        quant_ok = True\nexcept Exception as e:\n    quant_notes = str(e)\n    with open(ART_DIR / \"export_int8_error.txt\", \"w\") as f:\n        f.write(quant_notes)\n\ndef torch_profile_latency(model: nn.Module, precision: str, warmup: int = 20, repeats: int = 100):\n    proc = psutil.Process(os.getpid())\n    rss0 = proc.memory_info().rss\n\n    if torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats()\n        torch.cuda.synchronize()\n\n    if precision == \"fp16_amp\":\n        m = model.to(DEVICE).eval().float()\n        rgb = x_rgb_fp32\n        dep = x_dep_fp32\n        use_amp = True\n    elif precision == \"fp16\":\n        m = model.to(DEVICE).eval().half()\n        rgb = x_rgb_fp16\n        dep = x_dep_fp16\n        use_amp = False\n    else:\n        m = model.to(DEVICE).eval().float()\n        rgb = x_rgb_fp32\n        dep = x_dep_fp32\n        use_amp = False\n\n    with torch.no_grad():\n        for _ in range(warmup):\n            if use_amp and torch.cuda.is_available():\n                with torch.amp.autocast(\"cuda\", enabled=True):\n                    _ = m(rgb, dep)\n            else:\n                _ = m(rgb, dep)\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n\n        times = []\n        for _ in range(repeats):\n            t0 = time.perf_counter()\n            if use_amp and torch.cuda.is_available():\n                with torch.amp.autocast(\"cuda\", enabled=True):\n                    _ = m(rgb, dep)\n            else:\n                _ = m(rgb, dep)\n            if torch.cuda.is_available():\n                torch.cuda.synchronize()\n            t1 = time.perf_counter()\n            times.append((t1 - t0) * 1000.0)\n\n    rss1 = proc.memory_info().rss\n    peak_ram_mb = (max(rss0, rss1) - min(rss0, rss1)) / (1024**2)\n    peak_vram_mb = (torch.cuda.max_memory_allocated() / (1024**2)) if torch.cuda.is_available() else 0.0\n\n    lat_mean = float(np.mean(times))\n    lat_std = float(np.std(times))\n    fps = float(1000.0 / max(1e-6, lat_mean))\n    return lat_mean, lat_std, fps, float(peak_ram_mb), float(peak_vram_mb)\n\nprofiles = []\n\nlat, std, fps, ram, vram = torch_profile_latency(proposed, \"fp32\", warmup=20, repeats=100)\nprofiles.append({\n    \"variant\": \"pytorch\",\n    \"precision\": \"fp32\",\n    \"input_resolution\": f\"{TARGET_HW[0]}x{TARGET_HW[1]}\",\n    \"latency_ms_mean\": lat,\n    \"latency_ms_std\": std,\n    \"fps\": fps,\n    \"peak_ram_mb\": ram,\n    \"peak_vram_mb\": vram\n})\n\nlat, std, fps, ram, vram = torch_profile_latency(proposed, \"fp16_amp\", warmup=20, repeats=100)\nprofiles.append({\n    \"variant\": \"pytorch\",\n    \"precision\": \"fp16_amp\",\n    \"input_resolution\": f\"{TARGET_HW[0]}x{TARGET_HW[1]}\",\n    \"latency_ms_mean\": lat,\n    \"latency_ms_std\": std,\n    \"fps\": fps,\n    \"peak_ram_mb\": ram,\n    \"peak_vram_mb\": vram\n})\n\nTAB_01_EDGE = PHASE6_DIR / \"tab_01_edge_inference_profile.csv\"\ndf_edge = pd.DataFrame(profiles)\ndf_edge.to_csv(TAB_01_EDGE, index=False)\n\nexports = []\nexports.append({\n    \"artifact_name\": \"checkpoint_best\",\n    \"format\": \"pt\",\n    \"path\": str(Path(ck_p)),\n    \"file_size_mb\": file_size_mb(Path(ck_p)),\n    \"opset\": \"\",\n    \"quantized\": False,\n    \"notes\": \"best checkpoint from phase4 index\"\n})\nexports.append({\n    \"artifact_name\": \"onnx_fp32\",\n    \"format\": \"onnx\",\n    \"path\": str(ONNX_FP32),\n    \"file_size_mb\": file_size_mb(ONNX_FP32),\n    \"opset\": 17 if export_ok_fp32 else \"\",\n    \"quantized\": False,\n    \"notes\": \"two-input onnx: rgb, depth\"\n})\nexports.append({\n    \"artifact_name\": \"onnx_fp16\",\n    \"format\": \"onnx\",\n    \"path\": str(ONNX_FP16),\n    \"file_size_mb\": file_size_mb(ONNX_FP16),\n    \"opset\": 17 if export_ok_fp16 else \"\",\n    \"quantized\": False,\n    \"notes\": \"exported from half model\"\n})\nexports.append({\n    \"artifact_name\": \"onnx_int8_dynamic\",\n    \"format\": \"onnx\",\n    \"path\": str(ONNX_INT8),\n    \"file_size_mb\": file_size_mb(ONNX_INT8),\n    \"opset\": \"\",\n    \"quantized\": bool(quant_ok),\n    \"notes\": \"onnxruntime dynamic quantization\" if quant_ok else f\"not created: {quant_notes[:160]}\"\n})\n\nTAB_02_EXPORTS = PHASE6_DIR / \"tab_02_export_artifacts_table.csv\"\ndf_exports = pd.DataFrame(exports)\ndf_exports.to_csv(TAB_02_EXPORTS, index=False)\n\nFIG_01_LAT = PHASE6_DIR / \"fig_01_latency_fps_tradeoff.png\"\nplt.figure(figsize=(8.4, 4.6))\nx = np.arange(len(df_edge))\nplt.bar(x - 0.2, df_edge[\"latency_ms_mean\"].values, width=0.4, label=\"Latency (ms)\")\nax1 = plt.gca()\nax2 = ax1.twinx()\nax2.plot(x + 0.2, df_edge[\"fps\"].values, marker=\"o\", label=\"FPS\")\nax1.set_xticks(x)\nax1.set_xticklabels((df_edge[\"variant\"] + \"|\" + df_edge[\"precision\"]).values, rotation=15, ha=\"right\")\nax1.set_ylabel(\"Latency (ms)\")\nax2.set_ylabel(\"FPS\")\nax1.set_title(\"Latency and FPS tradeoff\")\nlines0, labels0 = ax1.get_legend_handles_labels()\nlines1, labels1 = ax2.get_legend_handles_labels()\nax1.legend(lines0 + lines1, labels0 + labels1, loc=\"upper right\")\nsave_fig(FIG_01_LAT, dpi=300)\n\nFIG_02_MEM = PHASE6_DIR / \"fig_02_memory_and_modelsize.png\"\nmodel_sz = file_size_mb(Path(ck_p))\nplt.figure(figsize=(8.4, 4.6))\nx = np.arange(len(df_edge))\nplt.bar(x - 0.2, df_edge[\"peak_vram_mb\"].values, width=0.4, label=\"Peak VRAM (MB)\")\nplt.bar(x + 0.2, df_edge[\"peak_ram_mb\"].values, width=0.4, label=\"Peak RAM delta (MB)\")\nplt.xticks(x, (df_edge[\"variant\"] + \"|\" + df_edge[\"precision\"]).values, rotation=15, ha=\"right\")\nplt.ylabel(\"MB\")\nplt.title(f\"Memory profile and model size (ckpt {model_sz:.1f} MB)\")\nplt.legend()\nsave_fig(FIG_02_MEM, dpi=300)\n\nFIG_03_PIPE = PHASE6_DIR / \"fig_03_final_pipeline_overview.png\"\nplt.figure(figsize=(11.5, 3.4))\nax = plt.gca()\nax.axis(\"off\")\nboxes = [\n    (\"ISOD dataset\\nPhase 1–2\", 0.02, 0.35, 0.16, 0.35),\n    (\"Preprocess +\\nsite split\\nPhase 3\", 0.22, 0.35, 0.16, 0.35),\n    (\"Train baselines +\\nproposed ViT fusion\\nPhase 4\", 0.42, 0.35, 0.20, 0.35),\n    (\"Eval + robustness +\\nexplainability\\nPhase 5\", 0.66, 0.35, 0.18, 0.35),\n    (\"ONNX export +\\nprofiling package\\nPhase 6\", 0.86, 0.35, 0.12, 0.35),\n]\nfor text, x0, y0, w, h in boxes:\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, fill=False, linewidth=2))\n    ax.text(x0 + w/2, y0 + h/2, text, ha=\"center\", va=\"center\")\nfor i in range(len(boxes)-1):\n    x0 = boxes[i][1] + boxes[i][3]\n    y0 = boxes[i][2] + boxes[i][4]/2\n    x1 = boxes[i+1][1]\n    y1 = boxes[i+1][2] + boxes[i+1][4]/2\n    ax.annotate(\"\", xy=(x1, y1), xytext=(x0, y0), arrowprops=dict(arrowstyle=\"->\", lw=2))\nsave_fig(FIG_03_PIPE, dpi=300)\n\nTAB_02_MAIN = PHASE5_DIR / \"tab_02_main_results_table.csv\"\nTAB_04_ROB = PHASE5_DIR / \"tab_04_robustness_table.csv\"\ndf_main = pd.read_csv(TAB_02_MAIN) if TAB_02_MAIN.exists() else pd.DataFrame()\ndf_rob = pd.read_csv(TAB_04_ROB) if TAB_04_ROB.exists() else pd.DataFrame()\n\ndef key_snapshot(df_main: pd.DataFrame, df_rob: pd.DataFrame, df_edge: pd.DataFrame):\n    out = {}\n    if len(df_main):\n        row = df_main[df_main[\"method\"] == \"proposed_vit_fusion\"]\n        if len(row):\n            out[\"clean_miou\"] = float(row.iloc[0][\"miou\"])\n            out[\"clean_dice\"] = float(row.iloc[0][\"dice\"])\n    if len(df_rob):\n        rp = df_rob[(df_rob[\"model\"] == \"proposed_vit_fusion\") & (df_rob[\"condition\"] == \"depth_missing\")]\n        if len(rp):\n            out[\"depth_missing_miou\"] = float(rp.sort_values(\"severity\").iloc[0][\"miou\"])\n    if len(df_edge):\n        fp32 = df_edge[(df_edge[\"variant\"] == \"pytorch\") & (df_edge[\"precision\"] == \"fp32\")]\n        fp16 = df_edge[(df_edge[\"variant\"] == \"pytorch\") & (df_edge[\"precision\"] == \"fp16_amp\")]\n        if len(fp32):\n            out[\"lat_ms_fp32\"] = float(fp32.iloc[0][\"latency_ms_mean\"])\n            out[\"fps_fp32\"] = float(fp32.iloc[0][\"fps\"])\n        if len(fp16):\n            out[\"lat_ms_fp16\"] = float(fp16.iloc[0][\"latency_ms_mean\"])\n            out[\"fps_fp16\"] = float(fp16.iloc[0][\"fps\"])\n    out[\"ckpt_mb\"] = file_size_mb(Path(ck_p))\n    out[\"onnx_fp32_mb\"] = file_size_mb(ONNX_FP32) if export_ok_fp32 else float(\"nan\")\n    out[\"onnx_fp16_mb\"] = file_size_mb(ONNX_FP16) if export_ok_fp16 else float(\"nan\")\n    out[\"params_m\"] = params_m(proposed)\n    return out\n\nsnap = key_snapshot(df_main, df_rob, df_edge)\n\nFIG_04_SNAP = PHASE6_DIR / \"fig_04_key_results_snapshot.png\"\nplt.figure(figsize=(10.5, 4.8))\nax = plt.gca()\nax.axis(\"off\")\nlines = [\n    f\"Proposed (ViT fusion) key snapshot\",\n    f\"Clean test: mIoU={snap.get('clean_miou', float('nan')):.4f}  Dice={snap.get('clean_dice', float('nan')):.4f}\",\n    f\"Robustness depth-missing: mIoU={snap.get('depth_missing_miou', float('nan')):.4f}\",\n    f\"Edge PyTorch FP32: {snap.get('lat_ms_fp32', float('nan')):.2f} ms  {snap.get('fps_fp32', float('nan')):.1f} FPS\",\n    f\"Edge PyTorch FP16 AMP: {snap.get('lat_ms_fp16', float('nan')):.2f} ms  {snap.get('fps_fp16', float('nan')):.1f} FPS\",\n    f\"Model: params={snap.get('params_m', float('nan')):.2f} M  ckpt={snap.get('ckpt_mb', float('nan')):.1f} MB  onnx_fp32={snap.get('onnx_fp32_mb', float('nan')):.1f} MB\",\n]\nax.text(0.02, 0.85, \"\\n\".join(lines), va=\"top\", ha=\"left\", fontsize=12)\nsave_fig(FIG_04_SNAP, dpi=300)\n\ndef build_artifact_index(results_root: Path):\n    rows = []\n    for phase_dir in sorted(results_root.glob(\"*\")):\n        if not phase_dir.is_dir():\n            continue\n        phase = phase_dir.name.split(\"_\")[0]\n        for p in sorted(phase_dir.rglob(\"*\")):\n            if p.is_dir():\n                continue\n            rel = p.relative_to(results_root)\n            fn = p.name\n            ext = p.suffix.lower()\n            if ext in [\".png\", \".jpg\", \".jpeg\", \".pdf\"]:\n                atype = \"figure\"\n            elif ext in [\".csv\"]:\n                atype = \"table_csv\"\n            elif ext in [\".pt\", \".pth\", \".ckpt\", \".safetensors\", \".onnx\"]:\n                atype = \"model_artifact\"\n            else:\n                atype = \"other\"\n            rows.append({\n                \"phase\": phase,\n                \"artifact_type\": atype,\n                \"filename\": fn,\n                \"relative_path\": str(rel),\n                \"description\": \"\"\n            })\n    return pd.DataFrame(rows)\n\nTAB_03_INDEX = PHASE6_DIR / \"tab_03_final_artifact_index.csv\"\ndf_index = build_artifact_index(RESULTS_ROOT)\ndf_index.to_csv(TAB_03_INDEX, index=False)\n\nTAB_04_CHECK = PHASE6_DIR / \"tab_04_reproducibility_checklist.csv\"\ncheck_rows = [\n    {\"item\": \"Dataset manifest and integrity logs available\", \"status\": \"ok\" if (RESULTS_ROOT/\"01_data_integrity_and_profile\").exists() else \"missing\", \"details\": \"Phase 1 outputs directory present\"},\n    {\"item\": \"Sensor alignment metrics available\", \"status\": \"ok\" if (RESULTS_ROOT/\"02_sensor_alignment_and_quality\").exists() else \"missing\", \"details\": \"Phase 2 outputs directory present\"},\n    {\"item\": \"Preprocess config and site split manifest available\", \"status\": \"ok\" if (RESULTS_ROOT/\"03_preprocess_and_split_protocol\"/\"tab_03_split_manifest.csv\").exists() else \"missing\", \"details\": \"Phase 3 split manifest\"},\n    {\"item\": \"Best checkpoint index available\", \"status\": \"ok\" if (PHASE4_DIR/\"tab_03_best_checkpoint_index.csv\").exists() else \"missing\", \"details\": \"Phase 4 checkpoint index\"},\n    {\"item\": \"Test metrics CSV source available\", \"status\": \"ok\" if (PHASE5_DIR/\"tab_01_test_results_full.csv\").exists() else \"missing\", \"details\": \"Phase 5 full test results\"},\n    {\"item\": \"Edge profile CSV source available\", \"status\": \"ok\" if (PHASE6_DIR/\"tab_01_edge_inference_profile.csv\").exists() else \"missing\", \"details\": \"Phase 6 edge inference profile\"},\n    {\"item\": \"ONNX export available\", \"status\": \"ok\" if (ONNX_FP32.exists() or ONNX_FP16.exists()) else \"missing\", \"details\": \"Phase 6 ONNX artifacts\"},\n    {\"item\": \"Final artifact index table available\", \"status\": \"ok\" if TAB_03_INDEX.exists() else \"missing\", \"details\": \"Phase 6 index CSV\"},\n]\ndf_check = pd.DataFrame(check_rows)\ndf_check.to_csv(TAB_04_CHECK, index=False)\n\nTAB_01_EDGE = PHASE6_DIR / \"tab_01_edge_inference_profile.csv\"\nTAB_02_EXPORTS = PHASE6_DIR / \"tab_02_export_artifacts_table.csv\"\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nRESULT_DIR = Path(\"/kaggle/working/derived_image_operations\")\nRESULT_DIR.mkdir(parents=True, exist_ok=True)\n\nMANIFEST = Path(\"/kaggle/working/results/01_data_integrity_and_profile/tab_01_dataset_manifest_valid_only.csv\")\ndf = pd.read_csv(MANIFEST).reset_index(drop=True)\n\nSEED = 1337\nrng = np.random.default_rng(SEED)\n\nsamples = []\nfor site_id, g in df.groupby(\"site_id\"):\n    if len(g) > 0:\n        samples.append(g.sample(n=1, random_state=int(rng.integers(1e9))))\n\nSAMPLES = pd.concat(samples, ignore_index=True)\n\nif len(SAMPLES) > 10:\n    SAMPLES = SAMPLES.sample(n=10, random_state=SEED).reset_index(drop=True)\n\ndef read_rgb(p):\n    return cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2RGB)\n\ndef read_depth(p):\n    d = cv2.imread(p, cv2.IMREAD_UNCHANGED).astype(np.float32)\n    d = (d - d.min()) / (d.max() - d.min() + 1e-6)\n    return d\n\ndef read_mask(p):\n    return cv2.imread(p, cv2.IMREAD_GRAYSCALE) > 0\n\nfor _, row in SAMPLES.iterrows():\n    rgb = read_rgb(row[\"rgb_path\"])\n    depth = read_depth(row[\"depth_path\"])\n    mask = read_mask(row[\"mask_path\"])\n\n    gray = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)\n    blur = cv2.GaussianBlur(rgb, (9, 9), 0)\n    edges = cv2.Canny(gray, 80, 160)\n\n    low_light = np.clip(rgb * 0.4, 0, 255).astype(np.uint8)\n\n    contrast = cv2.normalize(rgb, None, 0, 255, cv2.NORM_MINMAX)\n\n    depth_color = cv2.applyColorMap((depth * 255).astype(np.uint8), cv2.COLORMAP_INFERNO)\n\n    mask_overlay = rgb.copy()\n    mask_overlay[mask] = [255, 0, 0]\n\n    rgb_depth_overlay = (0.6 * rgb + 0.4 * depth_color).astype(np.uint8)\n\n    final_input = cv2.normalize(\n        (0.7 * contrast + 0.3 * depth_color).astype(np.uint8),\n        None, 0, 255, cv2.NORM_MINMAX\n    )\n\n    imgs = [\n        rgb,\n        gray,\n        blur,\n        edges,\n        low_light,\n        contrast,\n        depth_color,\n        mask_overlay,\n        rgb_depth_overlay,\n        final_input\n    ]\n\n    titles = [\n        \"RGB\",\n        \"Grayscale\",\n        \"Gaussian Blur\",\n        \"Edges\",\n        \"Low-light\",\n        \"Contrast Stretch\",\n        \"Depth (Color)\",\n        \"Mask Overlay\",\n        \"RGB + Depth\",\n        \"RGB\"\n    ]\n\n    plt.figure(figsize=(14, 6))\n    for i, (im, t) in enumerate(zip(imgs, titles)):\n        plt.subplot(2, 5, i + 1)\n        if im.ndim == 2:\n            plt.imshow(im, cmap=\"gray\")\n        else:\n            plt.imshow(im)\n        plt.title(t, fontsize=9)\n        plt.axis(\"off\")\n\n    out_path = RESULT_DIR / f\"{row['sample_id']}_operations.png\"\n    plt.tight_layout()\n    plt.savefig(out_path, dpi=300)\n    plt.close()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}